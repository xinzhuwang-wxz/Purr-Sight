"""
éŸ³é¢‘é¢„å¤„ç†æ¨¡å—ï¼šæ”¯æŒæ–‡ä»¶å’Œå†…å­˜æµè¾“å…¥ï¼Œè¿”å›žæ¢…å°”é¢‘è°±ç‰¹å¾

ä½¿ç”¨PANNså®˜æ–¹çš„æ¢…å°”é¢‘è°±æå–å™¨ï¼ˆä»Žæƒé‡æ–‡ä»¶åŠ è½½logmel_extractor.melWï¼‰ã€‚

åŒ…å«ï¼š
- _AudioProcessor: éŸ³é¢‘é¢„å¤„ç†å™¨ç±»ï¼ˆFail-Safeç‰ˆæœ¬ï¼‰

å‚è€ƒä»“åº“ï¼š
- pytorch/audio (torchaudio.pipelines.YAMNET)
"""

import torch
import torchaudio
import torchaudio.transforms as T
import numpy as np
from io import BytesIO
from typing import Union, Optional
import tempfile
import os
import subprocess
from pathlib import Path
from purrsight.config import ROOT_DIR


class _AudioProcessor:
    """
    éŸ³é¢‘é¢„å¤„ç†å™¨ï¼šæ”¯æŒæ–‡ä»¶è·¯å¾„å’Œå†…å­˜æµè¾“å…¥ï¼ˆFail-Safeç‰ˆæœ¬ï¼‰
    
    ä½¿ç”¨PANNså®˜æ–¹çš„æ¢…å°”é¢‘è°±æå–å™¨ï¼ˆä»Žæƒé‡æ–‡ä»¶åŠ è½½ï¼‰ï¼Œè¾“å‡ºæ ‡å‡†æ¢…å°”é¢‘è°±ç‰¹å¾(64, 256)ã€‚
    
    Attributes:
        sample_rate: ç›®æ ‡é‡‡æ ·çŽ‡
        n_mels: æ¢…å°”æ»¤æ³¢å™¨ç»„æ•°é‡
        n_fft: FFTçª—å£å¤§å°
        hop_length: å¸§ç§»
        stft: STFTå˜æ¢å™¨
        amplitude_to_db: å¹…åº¦åˆ°dBå˜æ¢å™¨
        mel_transform: æ¢…å°”é¢‘è°±å˜æ¢å™¨
        use_official_mel: æ˜¯å¦ä½¿ç”¨å®˜æ–¹æ¢…å°”æ»¤æ³¢å™¨ç»„
    """

    def __init__(
        self, 
        sample_rate: int = 16000, 
        n_mels: int = 64, 
        n_fft: int = 512, 
        hop_length: int = 160,
        weight_path: Optional[Union[str, Path]] = None
    ):
        """
        åˆå§‹åŒ–éŸ³é¢‘é¢„å¤„ç†å‚æ•°
        
        Args:
            sample_rate: ç›®æ ‡é‡‡æ ·çŽ‡ï¼Œé»˜è®¤16kHzï¼ˆä¸ŽPANNså®˜æ–¹ä¸€è‡´ï¼‰
            n_mels: æ¢…å°”æ»¤æ³¢å™¨ç»„æ•°é‡ï¼Œé»˜è®¤64ï¼ˆä¸ŽPANNså®˜æ–¹ä¸€è‡´ï¼‰
            n_fft: FFTçª—å£å¤§å°ï¼Œé»˜è®¤512ï¼ˆä¸ŽPANNså®˜æ–¹window_sizeä¸€è‡´ï¼‰
            hop_length: å¸§ç§»ï¼Œé»˜è®¤160ï¼ˆä¸ŽPANNså®˜æ–¹hop_sizeä¸€è‡´ï¼‰
            weight_path: PANNsæƒé‡æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºŽæå–å®˜æ–¹æ¢…å°”æ»¤æ³¢å™¨ç»„
        """
        self.sample_rate = sample_rate
        self.n_mels = n_mels
        self.n_fft = n_fft
        self.hop_length = hop_length
        
        self._load_panns_mel_extractor(weight_path)
        
        self.stft = T.Spectrogram(
            n_fft=n_fft,
            hop_length=hop_length,
            power=1.0,
            normalized=False
        )
        
        self.amplitude_to_db = T.AmplitudeToDB()  # å¯¹æ•°å˜æ¢
    
    def _load_panns_mel_extractor(self, weight_path: Optional[Union[str, Path]]):
        """
        ä»ŽPANNsæƒé‡æ–‡ä»¶ä¸­åŠ è½½å®˜æ–¹çš„æ¢…å°”é¢‘è°±æå–å™¨
        
        Args:
            weight_path: æƒé‡æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æžœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        if weight_path is None:
            weight_path = Path(ROOT_DIR, "models/panns/cnn14_light_16k.pth")
        else:
            weight_path = Path(weight_path)
        
        if not weight_path.exists():
            self.use_official_mel = False
            self.mel_transform = T.MelSpectrogram(
                sample_rate=self.sample_rate,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                n_mels=self.n_mels,
                f_min=50,
                f_max=8000
            )
            return
        
        try:
            state_dict = torch.load(weight_path, map_location="cpu", weights_only=False)
            
            if 'model' in state_dict:
                model_state = state_dict['model']
            else:
                model_state = state_dict
            
            if 'logmel_extractor.melW' in model_state:
                self.melW = model_state['logmel_extractor.melW']
                self.use_official_mel = True
            else:
                self.use_official_mel = False
                self.mel_transform = T.MelSpectrogram(
                    sample_rate=self.sample_rate,
                    n_fft=self.n_fft,
                    hop_length=self.hop_length,
                    n_mels=self.n_mels,
                    f_min=50,
                    f_max=8000
                )
        except Exception:
            self.use_official_mel = False
            self.mel_transform = T.MelSpectrogram(
                sample_rate=self.sample_rate,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                n_mels=self.n_mels,
                f_min=50,
                f_max=8000
            )
    
    def _apply_official_mel_filterbank(self, spectrogram: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨PANNså®˜æ–¹çš„æ¢…å°”æ»¤æ³¢å™¨ç»„æå–æ¢…å°”é¢‘è°±
        
        Args:
            spectrogram: å¹…åº¦é¢‘è°±å›¾ï¼Œå½¢çŠ¶ (B, freq_bins, time)
        
        Returns:
            æ¢…å°”é¢‘è°±ï¼Œå½¢çŠ¶ (B, n_mels, time)
        """
        mel_spec = torch.matmul(spectrogram.transpose(1, 2), self.melW).transpose(1, 2)
        return mel_spec

    def process_audio(self, audio_data: Union[str, BytesIO]) -> np.ndarray:
        """
        éŸ³é¢‘é¢„å¤„ç†ï¼šé‡é‡‡æ ·â†’STFTâ†’æ¢…å°”æ»¤æ³¢å™¨ç»„â†’å¯¹æ•°å˜æ¢â†’é™éŸ³è£å‰ª
        
        ðŸ”§ ä¿®å¤ï¼šç¦»çº¿é¢„å¤„ç†æ¨¡å¼ä¸‹ï¼Œå¦‚æžœéŸ³é¢‘æ— æ³•åŠ è½½ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›žé›¶å‘é‡ã€‚
        è¿™æ ·å¯ä»¥ç¡®ä¿é¢„å¤„ç†çš„æ•°æ®éƒ½æ˜¯å¯ç”¨çš„ï¼Œæ— æ³•åŠ è½½çš„æ ·æœ¬ä¼šè¢«è·³è¿‡ã€‚
        
        Args:
            audio_data: æ–‡ä»¶è·¯å¾„æˆ–å†…å­˜æµ(BytesIO)
        
        Returns:
            mel_spec: æ¢…å°”é¢‘è°±ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º(64, 256)ï¼Œdtype=float32
        
        Raises:
            ValueError: å½“éŸ³é¢‘æ–‡ä»¶æ— æ³•åŠ è½½æˆ–å¤„ç†å¤±è´¥æ—¶
        """
        # åŠ è½½éŸ³é¢‘ï¼ˆå¦‚æžœå¤±è´¥ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
        waveform, sample_rate = self._load_audio(audio_data)

        # å¤„ç†éŸ³é¢‘
        if sample_rate != self.sample_rate:
            resampler = T.Resample(sample_rate, self.sample_rate)
            waveform = resampler(waveform)

        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        waveform = self._trim_silence(waveform)

        if self.use_official_mel:
            spectrogram = self.stft(waveform)
            mel_spec = self._apply_official_mel_filterbank(spectrogram)
        else:
            mel_spec = self.mel_transform(waveform)

        mel_spec = self.amplitude_to_db(mel_spec)

        target_time = 256
        current_time = mel_spec.shape[2]

        if current_time > target_time:
            start = (current_time - target_time) // 2
            mel_spec = mel_spec[:, :, start:start + target_time]
        else:
            pad_width = target_time - current_time
            mel_spec = torch.nn.functional.pad(mel_spec, (0, pad_width), mode='constant', value=0)

        mel_spec = mel_spec.squeeze(0)
        return mel_spec.numpy()

    def _load_audio(self, audio_data: Union[str, BytesIO]) -> tuple[torch.Tensor, int]:
        """
        åŠ è½½éŸ³é¢‘æ•°æ®ï¼šæ”¯æŒæ–‡ä»¶è·¯å¾„å’Œå†…å­˜æµ
        
        è‡ªåŠ¨å¤„ç†ä¸æ”¯æŒçš„æ ¼å¼ï¼ˆå¦‚m4aï¼‰ï¼Œä½¿ç”¨ffmpegè½¬æ¢ä¸ºwavã€‚
        
        Args:
            audio_data: æ–‡ä»¶è·¯å¾„æˆ–BytesIOå¯¹è±¡
        
        Returns:
            (waveform, sample_rate)å…ƒç»„ï¼š
            - waveform: éŸ³é¢‘æ³¢å½¢tensorï¼Œå½¢çŠ¶ä¸º(channels, samples)
            - sample_rate: é‡‡æ ·çŽ‡
        
        Raises:
            ValueError: å½“éŸ³é¢‘æ–‡ä»¶æ— æ³•åŠ è½½æˆ–è½¬æ¢æ—¶
        """
        if isinstance(audio_data, str):
            audio_path = Path(audio_data)
            
            try:
                waveform, sample_rate = torchaudio.load(audio_path)
                return waveform, sample_rate
            except Exception as e:
                ext = audio_path.suffix.lower()
                unsupported_formats = {'.m4a', '.aac', '.mp3', '.flac', '.ogg', '.opus'}
                error_str = str(e).lower()
                
                # ðŸ”§ ä¿®å¤ï¼šå¦‚æžœ torchaudio åŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ ffmpeg è½¬æ¢
                # åŒ…æ‹¬ .wav æ–‡ä»¶ï¼ˆå¯èƒ½æ ¼å¼ä¸æ ‡å‡†ã€æŸåæˆ–ç¼–ç é—®é¢˜ï¼‰
                # æ£€æŸ¥é”™è¯¯ä¿¡æ¯ä¸­æ˜¯å¦åŒ…å«æ ¼å¼ç›¸å…³çš„å…³é”®è¯
                should_use_ffmpeg = (
                    ext in unsupported_formats or 
                    'format' in error_str or 
                    'not recognised' in error_str or
                    'not recognized' in error_str or
                    'unsupported' in error_str or
                    'codec' in error_str or
                    'decoder' in error_str
                )
                
                if should_use_ffmpeg:
                    temp_wav = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                    temp_wav_path = temp_wav.name
                    temp_wav.close()
                    
                    try:
                        result = subprocess.run(
                            ['ffmpeg', '-i', str(audio_path), '-y', 
                             '-ar', '16000',
                             '-ac', '1',
                             '-f', 'wav', 
                             '-loglevel', 'error',
                             temp_wav_path],
                            capture_output=True,
                            check=True,
                            timeout=30
                        )
                        
                        if not os.path.exists(temp_wav_path) or os.path.getsize(temp_wav_path) == 0:
                            raise ValueError(f"ffmpegè½¬æ¢åŽçš„æ–‡ä»¶ä¸ºç©º: {audio_path}")
                        
                        waveform, sample_rate = torchaudio.load(temp_wav_path)
                        return waveform, sample_rate
                    except subprocess.CalledProcessError as ffmpeg_error:
                        error_msg = ffmpeg_error.stderr.decode('utf-8', errors='ignore') if ffmpeg_error.stderr else "æœªçŸ¥é”™è¯¯"
                        error_preview = error_msg.split('\n')[0] if error_msg else "æœªçŸ¥é”™è¯¯"
                        raise ValueError(f"æ— æ³•åŠ è½½éŸ³é¢‘æ–‡ä»¶ {audio_path}: ffmpegè½¬æ¢å¤±è´¥: {error_preview}") from e
                    except subprocess.TimeoutExpired:
                        raise ValueError(f"éŸ³é¢‘è½¬æ¢è¶…æ—¶: {audio_path}") from e
                    except FileNotFoundError:
                        raise FileNotFoundError(f"ffmpegæœªæ‰¾åˆ°ï¼Œæ— æ³•è½¬æ¢éŸ³é¢‘æ ¼å¼: {audio_path}") from e
                    finally:
                        if os.path.exists(temp_wav_path):
                            os.unlink(temp_wav_path)
                else:
                    raise ValueError(f"æ— æ³•åŠ è½½éŸ³é¢‘æ–‡ä»¶ {audio_path}: {e}") from e

        elif isinstance(audio_data, BytesIO):
            audio_data.seek(0)
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_data.read())
                temp_path = temp_file.name

            try:
                waveform, sample_rate = torchaudio.load(temp_path)
                return waveform, sample_rate
            except Exception as e:
                raise ValueError(f"æ— æ³•åŠ è½½éŸ³é¢‘å†…å­˜æµ: {str(e)}")
            finally:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„éŸ³é¢‘è¾“å…¥ç±»åž‹: {type(audio_data)}")

    def _trim_silence(self, waveform: torch.Tensor, threshold_db: float = -40) -> torch.Tensor:
        """
        è£å‰ªéŸ³é¢‘é¦–å°¾çš„é™éŸ³éƒ¨åˆ†
        
        Args:
            waveform: éŸ³é¢‘æ³¢å½¢tensorï¼Œå½¢çŠ¶ä¸º(channels, samples)
            threshold_db: é™éŸ³é˜ˆå€¼ï¼ˆdBï¼‰ï¼Œé»˜è®¤-40dB
        
        Returns:
            è£å‰ªåŽçš„éŸ³é¢‘æ³¢å½¢tensor
        """
        """
        é™éŸ³è£å‰ªï¼šç§»é™¤å‰åŽé™éŸ³æ®µ
        
        Args:
            waveform: éŸ³é¢‘æ³¢å½¢ï¼Œå½¢çŠ¶ä¸º(1, time)
            threshold_db: é™éŸ³é˜ˆå€¼(dB)ï¼Œé»˜è®¤-40
        
        Returns:
            è£å‰ªåŽçš„æ³¢å½¢
        """
        rms = torch.sqrt(torch.mean(waveform ** 2, dim=0))
        db = 20 * torch.log10(rms + 1e-10)
        mask = db > threshold_db
        indices = torch.where(mask)[0]

        if len(indices) == 0:
            return waveform

        start_idx = indices[0]
        end_idx = indices[-1] + 1
        return waveform[:, start_idx:end_idx]