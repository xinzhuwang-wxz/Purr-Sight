"""é¢„å¤„ç†æ¨¡å—å…¬å…±æ¥å£ï¼šå¤šæ¨¡æ€è¾“å…¥è‡ªåŠ¨è¯†åˆ«ä¸å¤„ç†ã€‚

æ”¯æŒå­—å…¸è¾“å…¥å’Œç›´æ¥è¾“å…¥ï¼Œé›†æˆæ¨¡æ€åŒé‡æ ¡éªŒã€åŠ¨æ€å…³é”®å¸§é€‰æ‹©ã€å†…å­˜æµéŸ³é¢‘å¤„ç†ã€‚

åŒ…å«ï¼š
- Preprocessor: å¤šæ¨¡æ€é¢„å¤„ç†å™¨ä¸»ç±»

åŠŸèƒ½ï¼š
- å•æ ·æœ¬é¢„å¤„ç†ï¼šprocess()
- Batché¢„å¤„ç†ï¼šprocess_batch()
- ç¦»çº¿é¢„å¤„ç†åŠ è½½ï¼šload_preprocessed()
- è§†é¢‘åˆ†è§£ï¼šè§†é¢‘æ–‡ä»¶è‡ªåŠ¨åˆ†è§£ä¸º IMAGE å’Œ AUDIO ç‰¹å¾
- Fail-Safe ç­–ç•¥ï¼šæ‰€æœ‰å¤±è´¥æƒ…å†µéƒ½ä¸ä¼šå¯¼è‡´ç¨‹åº crash
"""

import os
import subprocess
import torch
import numpy as np
from io import BytesIO
from PIL import Image
from typing import Union, Dict, Any, List
from pathlib import Path

from .image import _ImageProcessor
from .audio import _AudioProcessor
from .text import _TextProcessor
from purrsight.config import Modality, FeatureKey, ModalitySource
from purrsight.utils.logging import logger


class Preprocessor:
    """å¤šæ¨¡æ€é¢„å¤„ç†å™¨ï¼šç»Ÿä¸€æ¥å£å¤„ç†å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ã€æ–‡æœ¬è¾“å…¥ã€‚
    
    æ”¯æŒå­—å…¸è¾“å…¥å’Œç›´æ¥è¾“å…¥ï¼Œè¾“å‡ºæ ‡è®°åŒ–ç‰¹å¾å­—å…¸ï¼ˆnumpy æ•°ç»„ï¼‰ã€‚
    é¢„å¤„ç†è¿”å› numpy æ•°ç»„ä»¥èŠ‚çœå†…å­˜ï¼Œè¾“å…¥æ¨¡å‹å‰ä½¿ç”¨ to_tensor() è½¬æ¢ä¸º tensorã€‚
    
    Attributes:
        image_processor: å›¾åƒé¢„å¤„ç†å™¨å®ä¾‹ã€‚
        audio_processor: éŸ³é¢‘é¢„å¤„ç†å™¨å®ä¾‹ã€‚
        text_processor: æ–‡æœ¬é¢„å¤„ç†å™¨å®ä¾‹ã€‚
        _instance: å•ä¾‹å®ä¾‹ï¼ˆç±»å±æ€§ï¼‰ã€‚
    """
    
    _instance = None

    def __init__(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡æ€é¢„å¤„ç†å™¨ã€‚
        
        Raises:
            RuntimeError: å½“é¢„å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥æ—¶ã€‚
        """
        # System Check: æ£€æŸ¥å…³é”®ä¾èµ–å’Œæƒé‡æ˜¯å¦å­˜åœ¨
        import shutil
        if not shutil.which("ffmpeg"):
            logger.warning("ç³»ç»Ÿæœªæ£€æµ‹åˆ° ffmpegï¼è§†é¢‘/éŸ³é¢‘å¤„ç†å°†æ— æ³•æ­£å¸¸å·¥ä½œã€‚è¯·å®‰è£… ffmpegã€‚")
            
        try:
            self.image_processor = _ImageProcessor()
            self.audio_processor = _AudioProcessor()
            self.text_processor = _TextProcessor()
        except Exception as e:
            raise RuntimeError(f"åˆå§‹åŒ–é¢„å¤„ç†å™¨å¤±è´¥: {str(e)}")
    
    @classmethod
    def process(cls, input_data: Union[Dict[str, Any], str, Image.Image]) -> Dict[str, np.ndarray]:
        """ç±»æ–¹æ³•ï¼šä¾¿æ·æ¥å£ï¼Œæ— éœ€å®ä¾‹åŒ–å³å¯è°ƒç”¨ã€‚
        
        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œæ”¯æŒä¸‰ç§æ ¼å¼ï¼š
                - å­—å…¸ï¼š{"modality": data_path_or_content}
                - å­—ç¬¦ä¸²ï¼šæ–‡ä»¶è·¯å¾„ï¼ˆè‡ªåŠ¨æ¨æ–­æ¨¡æ€ï¼‰
                - PIL.Imageï¼šå›¾åƒå¯¹è±¡
        
        Returns:
            æ¨¡æ€æ ‡è®°åŒ–ç‰¹å¾å­—å…¸ï¼ˆnumpy æ•°ç»„ï¼‰ï¼Œä½¿ç”¨ FeatureKey è®¿é—®ã€‚
        
        Example:
            >>> features = Preprocessor.process({"text": "I love cat"})
            >>> features = Preprocessor.process("/path/to/image.jpg")
        """
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance._process(input_data)
    
    @classmethod
    def load_preprocessed(cls, preprocessed_files: Dict[str, str], base_dir: Path) -> Dict[str, np.ndarray]:
        """ç±»æ–¹æ³•ï¼šåŠ è½½é¢„å¤„ç†åçš„ç‰¹å¾æ–‡ä»¶ã€‚
        
        Args:
            preprocessed_files: é¢„å¤„ç†æ–‡ä»¶è·¯å¾„å­—å…¸ï¼Œé”®ä¸ºæ¨¡æ€åç§°ï¼Œå€¼ä¸ºç›¸å¯¹è·¯å¾„ã€‚
            base_dir: é¢„å¤„ç†æ–‡ä»¶çš„åŸºç¡€ç›®å½•ã€‚
            
        Returns:
            ç‰¹å¾å­—å…¸ï¼ˆnumpy æ•°ç»„ï¼‰ï¼Œæ ¼å¼ä¸ process() ç›¸åŒã€‚
        """
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance._load_preprocessed(preprocessed_files, base_dir)
    
    def _load_preprocessed(self, preprocessed_files: Dict[str, str], base_dir: Path) -> Dict[str, np.ndarray]:
        """åŠ è½½é¢„å¤„ç†åçš„ç‰¹å¾æ–‡ä»¶ã€‚
        
        Args:
            preprocessed_files: é¢„å¤„ç†æ–‡ä»¶è·¯å¾„å­—å…¸ã€‚
            base_dir: é¢„å¤„ç†æ–‡ä»¶çš„åŸºç¡€ç›®å½•ã€‚
            
        Returns:
            ç‰¹å¾å­—å…¸ï¼ˆnumpy æ•°ç»„ï¼‰ã€‚
            
        Raises:
            ValueError: å¦‚æœå…³é”®æ¨¡æ€æ–‡ä»¶ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ï¼‰æŸåã€‚
        """
        features = {}
        
        # åŠ è½½æ–‡æœ¬ç‰¹å¾
        if "text" in preprocessed_files:
            text_file = base_dir / preprocessed_files["text"]
            if text_file.exists():
                try:
                    # ğŸ”§ éªŒè¯2ï¼šç«‹åˆ»å…³é—­mmapï¼ˆéªŒè¯mmapæ–‡ä»¶å¥æŸ„æ³„æ¼é—®é¢˜ï¼‰
                    # ä¸è¦åšsizeåˆ¤æ–­ï¼Œå…ˆéªŒè¯ç¨³å®šæ€§
                    use_mmap = False  # ğŸ”§ éªŒè¯ï¼šä¸´æ—¶å…³é—­mmap
                    
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ·»åŠ mmap=Trueä»¥åŠ é€Ÿå¤§æ–‡ä»¶åŠ è½½ï¼Œå‡å°‘å†…å­˜æ‹·è´å¼€é”€
                    text_data = torch.load(text_file, map_location="cpu", mmap=use_mmap, weights_only=False)
                    features[FeatureKey.TEXT] = text_data["input_ids"].numpy()
                    features[FeatureKey.TEXT_ATTENTION_MASK] = text_data["attention_mask"].numpy()
                    
                    # ğŸ”§ ä¿®å¤ï¼šæ˜¾å¼åˆ é™¤å¼•ç”¨ï¼Œå¸®åŠ©GCå›æ”¶ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
                    del text_data
                except (EOFError, RuntimeError, ValueError) as e:
                    # ğŸ”§ ä¿®å¤ï¼šç¦»çº¿é¢„å¤„ç†æ¨¡å¼ä¸‹ï¼Œå¦‚æœå…³é”®æ¨¡æ€æ–‡ä»¶æŸåï¼ŒæŠ›å‡ºå¼‚å¸¸è·³è¿‡æ•´ä¸ªæ ·æœ¬
                    # æ–‡æœ¬æ˜¯å¿…éœ€çš„ï¼Œå¦‚æœæŸååº”è¯¥è·³è¿‡æ•´ä¸ªæ ·æœ¬
                    raise ValueError(f"æŸåçš„æ–‡æœ¬æ–‡ä»¶ {text_file}: {e}") from e
        
        # åŠ è½½å›¾åƒç‰¹å¾ï¼ˆå¯èƒ½æ˜¯è§†é¢‘å¸§ï¼‰
        if "image" in preprocessed_files:
            image_file = base_dir / preprocessed_files["image"]
            if image_file.exists():
                try:
                    # ğŸ”§ éªŒè¯2ï¼šç«‹åˆ»å…³é—­mmapï¼ˆéªŒè¯mmapæ–‡ä»¶å¥æŸ„æ³„æ¼é—®é¢˜ï¼‰
                    # ä¸è¦åšsizeåˆ¤æ–­ï¼Œå…ˆéªŒè¯ç¨³å®šæ€§
                    use_mmap = False  # ğŸ”§ éªŒè¯ï¼šä¸´æ—¶å…³é—­mmap
                    
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ·»åŠ mmap=Trueä»¥åŠ é€Ÿå¤§æ–‡ä»¶åŠ è½½ï¼Œå‡å°‘å†…å­˜æ‹·è´å¼€é”€
                    image_tensor = torch.load(image_file, map_location="cpu", mmap=use_mmap, weights_only=True)
                    features[FeatureKey.IMAGE] = image_tensor.numpy()
                    
                    # ğŸ”§ ä¿®å¤ï¼šæ˜¾å¼åˆ é™¤å¼•ç”¨ï¼Œå¸®åŠ©GCå›æ”¶ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
                    del image_tensor
                except (EOFError, RuntimeError, ValueError) as e:
                    # ğŸ”§ ä¿®å¤ï¼šç¦»çº¿é¢„å¤„ç†æ¨¡å¼ä¸‹ï¼Œå¦‚æœå…³é”®æ¨¡æ€æ–‡ä»¶æŸåï¼ŒæŠ›å‡ºå¼‚å¸¸è·³è¿‡æ•´ä¸ªæ ·æœ¬
                    # è€Œä¸æ˜¯è¿”å›éƒ¨åˆ†ç‰¹å¾ï¼ˆé¿å…é›¶å‘é‡å¡«å……ï¼‰
                    raise ValueError(f"æŸåçš„å›¾åƒæ–‡ä»¶ {image_file}: {e}") from e
        
        # åŠ è½½éŸ³é¢‘ç‰¹å¾
        if "audio" in preprocessed_files:
            audio_file = base_dir / preprocessed_files["audio"]
            if audio_file.exists():
                try:
                    # ğŸ”§ éªŒè¯2ï¼šç«‹åˆ»å…³é—­mmapï¼ˆéªŒè¯mmapæ–‡ä»¶å¥æŸ„æ³„æ¼é—®é¢˜ï¼‰
                    # ä¸è¦åšsizeåˆ¤æ–­ï¼Œå…ˆéªŒè¯ç¨³å®šæ€§
                    use_mmap = False  # ğŸ”§ éªŒè¯ï¼šä¸´æ—¶å…³é—­mmap
                    
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ·»åŠ mmap=Trueä»¥åŠ é€Ÿå¤§æ–‡ä»¶åŠ è½½ï¼Œå‡å°‘å†…å­˜æ‹·è´å¼€é”€
                    audio_tensor = torch.load(audio_file, map_location="cpu", mmap=use_mmap, weights_only=True)
                    features[FeatureKey.AUDIO] = audio_tensor.numpy()
                    
                    # ğŸ”§ ä¿®å¤ï¼šæ˜¾å¼åˆ é™¤å¼•ç”¨ï¼Œå¸®åŠ©GCå›æ”¶ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
                    del audio_tensor
                except (EOFError, RuntimeError, ValueError) as e:
                    # ğŸ”§ ä¿®å¤ï¼šç¦»çº¿é¢„å¤„ç†æ¨¡å¼ä¸‹ï¼Œå¦‚æœå…³é”®æ¨¡æ€æ–‡ä»¶æŸåï¼ŒæŠ›å‡ºå¼‚å¸¸è·³è¿‡æ•´ä¸ªæ ·æœ¬
                    # å¦‚æœç´¢å¼•æ–‡ä»¶ä¸­åŒ…å«audioï¼Œè¯´æ˜åŸå§‹æ ·æœ¬æœ‰audioï¼Œæ–‡ä»¶æŸååº”è¯¥è·³è¿‡æ•´ä¸ªæ ·æœ¬
                    raise ValueError(f"æŸåçš„éŸ³é¢‘æ–‡ä»¶ {audio_file}: {e}") from e
        
        # åŠ è½½è§†é¢‘å…ƒæ•°æ®
        if "video_metadata" in preprocessed_files:
            metadata_file = base_dir / preprocessed_files["video_metadata"]
            if metadata_file.exists():
                try:
                    # ğŸ”§ ä¿®å¤ï¼šå¯¹äºå°æ–‡ä»¶ï¼Œä¸ä½¿ç”¨mmapå¯èƒ½æ›´å¿«ï¼Œä¸”é¿å…æ–‡ä»¶å¥æŸ„æ³„æ¼
                    file_size_mb = metadata_file.stat().st_size / (1024 * 1024)
                    use_mmap = file_size_mb > 10  # åªå¯¹å¤§æ–‡ä»¶ä½¿ç”¨mmap
                    
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ·»åŠ mmap=Trueä»¥åŠ é€Ÿå¤§æ–‡ä»¶åŠ è½½ï¼Œå‡å°‘å†…å­˜æ‹·è´å¼€é”€
                    video_metadata = torch.load(metadata_file, map_location="cpu", mmap=use_mmap, weights_only=False)
                    features["_video_metadata"] = video_metadata
                    
                    # ğŸ”§ ä¿®å¤ï¼šæ˜¾å¼åˆ é™¤å¼•ç”¨ï¼Œå¸®åŠ©GCå›æ”¶ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
                    # æ³¨æ„ï¼švideo_metadataæ˜¯å­—å…¸ï¼Œä¸éœ€è¦delï¼Œå› ä¸ºå·²ç»èµ‹å€¼ç»™features
                except (EOFError, RuntimeError, ValueError) as e:
                    logger.warning(f"è·³è¿‡æŸåçš„è§†é¢‘å…ƒæ•°æ®æ–‡ä»¶ {metadata_file}: {e}")
        
        return features
    
    @classmethod
    def process_batch(cls, batch_inputs: List[Union[Dict[str, Any], str, Image.Image]], inference_mode: bool = False) -> Dict[str, np.ndarray]:
        """ç±»æ–¹æ³•ï¼šBatch é¢„å¤„ç†æ¥å£ï¼Œå¤„ç†å¤šä¸ªæ ·æœ¬ã€‚
        
        Args:
            batch_inputs: è¾“å…¥æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ”¯æŒä¸‰ç§æ ¼å¼ï¼š
                - å­—å…¸ï¼š{"modality": data_path_or_content}
                - å­—ç¬¦ä¸²ï¼šæ–‡ä»¶è·¯å¾„ï¼ˆè‡ªåŠ¨æ¨æ–­æ¨¡æ€ï¼‰
                - PIL.Imageï¼šå›¾åƒå¯¹è±¡
                ä¸åŒæ ·æœ¬å¯èƒ½æœ‰ä¸åŒçš„æ¨¡æ€ç»„åˆã€‚
            inference_mode: æ˜¯å¦ä¸ºæ¨ç†æ¨¡å¼ã€‚å¦‚æœæ˜¯ï¼Œåˆ™å°è¯•è·³è¿‡ä¸å¿…è¦çš„è®¡ç®—ï¼ˆå¦‚å•å¸§å›¾åƒæ‰©å±•ä¸º16å¸§ï¼‰ã€‚
        
        Returns:
            æ¨¡æ€æ ‡è®°åŒ–ç‰¹å¾å­—å…¸ï¼ˆnumpy æ•°ç»„ï¼‰ï¼Œä½¿ç”¨ FeatureKey è®¿é—®ã€‚
            æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯ batch æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (B, ...)ã€‚
        
        Example:
            >>> batch_inputs = [
            ...     {"text": "Cat playing", "image": "/path/to/cat1.jpg"},
            ...     {"text": "Cat sleeping"},
            ...     {"image": "/path/to/cat2.jpg"},
            ... ]
            >>> features = Preprocessor.process_batch(batch_inputs, inference_mode=True)
        """
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance._process_batch(batch_inputs, inference_mode=inference_mode)
    
    def _process(self, input_data: Union[Dict[str, Any], str, Image.Image]) -> Dict[str, np.ndarray]:
        """æ ¸å¿ƒå¤„ç†æ–¹æ³•ï¼šè¾“å…¥è§£æ -> æ¨¡æ€è¯†åˆ« -> ç‰¹å¾æå– -> è¿”å›æ ‡è®°åŒ–ç‰¹å¾ã€‚
        
        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œæ”¯æŒä¸‰ç§æ ¼å¼ï¼š
                - å­—å…¸ï¼š{"modality": data_path_or_content}
                - å­—ç¬¦ä¸²ï¼šæ–‡ä»¶è·¯å¾„ï¼ˆè‡ªåŠ¨æ¨æ–­æ¨¡æ€ï¼‰
                - PIL.Imageï¼šå›¾åƒå¯¹è±¡
        
        Returns:
            æ¨¡æ€æ ‡è®°åŒ–ç‰¹å¾å­—å…¸ï¼ˆnumpy æ•°ç»„ï¼‰ï¼Œä½¿ç”¨ FeatureKey è®¿é—®ã€‚
        """
        features = {}
        modality_sources = {}

        if isinstance(input_data, dict):
            for modality, data in input_data.items():
                modality_str = str(modality) if isinstance(modality, Modality) else modality
                
                if modality_str == Modality.TEXT:
                    text_result = self.text_processor.process_text(data)
                    features[FeatureKey.TEXT] = text_result["input_ids"]
                    features[FeatureKey.TEXT_ATTENTION_MASK] = text_result["attention_mask"]
                    modality_sources["text_source"] = ModalitySource.TEXT.value
                elif modality_str == Modality.IMAGE:
                    if isinstance(data, str):
                        img = Image.open(data)
                    else:
                        img = data
                    features[FeatureKey.IMAGE] = self.image_processor.process_image(img)
                    modality_sources["image_source"] = ModalitySource.IMAGE.value
                elif modality_str == Modality.VIDEO:
                    # ä½¿ç”¨å…¬å…±æ–¹æ³•å¤„ç†è§†é¢‘
                    video_features = self._process_video_to_features(data)
                    features.update(video_features)
                    # ä»video_metadataè·å–sourceä¿¡æ¯
                    if "_video_metadata" in video_features:
                        video_meta = video_features["_video_metadata"]
                        modality_sources["image_source"] = video_meta.get("image_source", ModalitySource.VIDEO.value)
                        modality_sources["audio_source"] = video_meta.get("audio_source")
                elif modality_str == Modality.AUDIO:
                    # ğŸ”§ ä¿®å¤ï¼šç¦»çº¿é¢„å¤„ç†æ¨¡å¼ä¸‹ï¼Œå¦‚æœéŸ³é¢‘æ— æ³•åŠ è½½ï¼ŒæŠ›å‡ºå¼‚å¸¸è·³è¿‡æ ·æœ¬
                    features[FeatureKey.AUDIO] = self.audio_processor.process_audio(data)
                    modality_sources["audio_source"] = ModalitySource.AUDIO.value

        elif isinstance(input_data, str):
            is_file_path = (
                os.path.exists(input_data) or 
                os.path.sep in input_data or 
                '.' in os.path.basename(input_data) and len(os.path.splitext(input_data)[1]) > 0
            )
            
            if not is_file_path:
                text_result = self.text_processor.process_text(input_data)
                features[FeatureKey.TEXT] = text_result["input_ids"]
                features[FeatureKey.TEXT_ATTENTION_MASK] = text_result["attention_mask"]
                modality_sources["text_source"] = ModalitySource.TEXT.value
            else:
                modality = self._infer_modality(input_data)

                if modality == Modality.TEXT:
                    with open(input_data, 'r', encoding='utf-8') as f:
                        text_content = f.read()
                    text_result = self.text_processor.process_text(text_content)
                    features[FeatureKey.TEXT] = text_result["input_ids"]
                    features[FeatureKey.TEXT_ATTENTION_MASK] = text_result["attention_mask"]
                    modality_sources["text_source"] = ModalitySource.TEXT.value
                elif modality == Modality.IMAGE:
                    img_path = Path(input_data)
                    
                    # åŸºæœ¬éªŒè¯ï¼šæ–‡ä»¶å­˜åœ¨å’Œå¤§å°
                    if not img_path.exists():
                        raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {input_data}")
                    
                    file_size = img_path.stat().st_size
                    if file_size == 0:
                        raise ValueError(f"å›¾åƒæ–‡ä»¶ä¸ºç©º: {input_data}")
                    
                    # åªä½¿ç”¨PILåŠ è½½ï¼Œå¤±è´¥ç›´æ¥æŠ›å¼‚å¸¸
                    try:
                        img = Image.open(input_data)
                        img.load()  # å¼ºåˆ¶è¯»å–å›¾åƒæ•°æ®ï¼ŒéªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                    except Exception as e:
                        raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒæ–‡ä»¶: {input_data}, é”™è¯¯: {e}") from e
                    
                    features[FeatureKey.IMAGE] = self.image_processor.process_image(img)
                    modality_sources["image_source"] = ModalitySource.IMAGE.value
                elif modality == Modality.VIDEO:
                    # ä½¿ç”¨å…¬å…±æ–¹æ³•å¤„ç†è§†é¢‘
                    video_features = self._process_video_to_features(input_data)
                    features.update(video_features)
                    # ä»video_metadataè·å–sourceä¿¡æ¯
                    if "_video_metadata" in video_features:
                        video_meta = video_features["_video_metadata"]
                        modality_sources["image_source"] = video_meta.get("image_source", ModalitySource.VIDEO.value)
                        modality_sources["audio_source"] = video_meta.get("audio_source")
                elif modality == Modality.AUDIO:
                    # ğŸ”§ ä¿®å¤ï¼šç¦»çº¿é¢„å¤„ç†æ¨¡å¼ä¸‹ï¼Œå¦‚æœéŸ³é¢‘æ— æ³•åŠ è½½ï¼ŒæŠ›å‡ºå¼‚å¸¸è·³è¿‡æ ·æœ¬
                    features[FeatureKey.AUDIO] = self.audio_processor.process_audio(input_data)
                    modality_sources["audio_source"] = ModalitySource.AUDIO.value

        elif isinstance(input_data, Image.Image):
            features[FeatureKey.IMAGE] = self.image_processor.process_image(input_data)
            modality_sources["image_source"] = ModalitySource.IMAGE.value
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {type(input_data)}")

        # æ·»åŠ modality_sourcesåˆ°features
        if modality_sources:
            features["_modality_sources"] = modality_sources

        return features
    
    def _process_batch(self, batch_inputs: List[Union[Dict[str, Any], str, Image.Image]], inference_mode: bool = False) -> Dict[str, np.ndarray]:
        """Batch é¢„å¤„ç†æ ¸å¿ƒæ–¹æ³•ï¼šå¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œè¿”å› batch æ ¼å¼çš„ç‰¹å¾ã€‚
        
        Args:
            batch_inputs: è¾“å…¥æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬å¯èƒ½æœ‰ä¸åŒçš„æ¨¡æ€ç»„åˆã€‚
            inference_mode: æ˜¯å¦ä¸ºæ¨ç†æ¨¡å¼ã€‚
        
        Returns:
            Batch æ ¼å¼çš„ç‰¹å¾å­—å…¸ï¼Œæ‰€æœ‰ç‰¹å¾å½¢çŠ¶ä¸º (B, ...)ã€‚
        """
        if len(batch_inputs) == 0:
            raise ValueError("batch_inputsä¸èƒ½ä¸ºç©º")
        
        batch_size = len(batch_inputs)
        
        # æ”¶é›†æ¯ä¸ªæ ·æœ¬çš„æ¨¡æ€æ•°æ®ï¼ˆä½¿ç”¨åˆ—è¡¨ç´¢å¼•å¯¹åº”æ ·æœ¬ï¼‰
        modality_data = {
            Modality.TEXT.value: [None] * batch_size,
            Modality.IMAGE.value: [None] * batch_size,
            Modality.VIDEO.value: [None] * batch_size,
            Modality.AUDIO.value: [None] * batch_size,
        }
        
        # è§£ææ¯ä¸ªæ ·æœ¬çš„è¾“å…¥
        for sample_idx, input_data in enumerate(batch_inputs):
            if isinstance(input_data, dict):
                for modality, data in input_data.items():
                    modality_str = str(modality) if isinstance(modality, Modality) else modality
                    if modality_str == Modality.TEXT.value:
                        modality_data[Modality.TEXT.value][sample_idx] = data
                    elif modality_str == Modality.IMAGE.value:
                        modality_data[Modality.IMAGE.value][sample_idx] = data
                    elif modality_str == Modality.VIDEO.value:
                        modality_data[Modality.VIDEO.value][sample_idx] = data
                    elif modality_str == Modality.AUDIO.value:
                        modality_data[Modality.AUDIO.value][sample_idx] = data
            
            elif isinstance(input_data, str):
                is_file_path = (
                    os.path.exists(input_data) or 
                    os.path.sep in input_data or 
                    '.' in os.path.basename(input_data) and len(os.path.splitext(input_data)[1]) > 0
                )
                
                if not is_file_path:
                    modality_data[Modality.TEXT.value][sample_idx] = input_data
                else:
                    modality = self._infer_modality(input_data)
                    if modality == Modality.TEXT:
                        with open(input_data, 'r', encoding='utf-8') as f:
                            text_content = f.read()
                        modality_data[Modality.TEXT.value][sample_idx] = text_content
                    elif modality == Modality.IMAGE:
                        modality_data[Modality.IMAGE.value][sample_idx] = input_data
                    elif modality == Modality.VIDEO:
                        modality_data[Modality.VIDEO.value][sample_idx] = input_data
                    elif modality == Modality.AUDIO:
                        modality_data[Modality.AUDIO.value][sample_idx] = input_data
            
            elif isinstance(input_data, Image.Image):
                modality_data[Modality.IMAGE.value][sample_idx] = input_data
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {type(input_data)}")
        
        # å¤„ç†æ¯ä¸ªæ¨¡æ€çš„batch
        features = {}
        
        # æ–‡æœ¬batchå¤„ç†
        if any(text is not None for text in modality_data[Modality.TEXT.value]):
            # å¯¹äºç¼ºå¤±çš„æ ·æœ¬ï¼Œç”¨ç©ºå­—ç¬¦ä¸²å¡«å……ï¼ˆä¼šè¢«paddingï¼‰
            text_list = []
            for text_data in modality_data[Modality.TEXT.value]:
                if text_data is not None:
                    text_list.append(text_data)
                else:
                    text_list.append("")  # ç©ºå­—ç¬¦ä¸²ä¼šè¢«padding
            
            text_result = self.text_processor.process_text(text_list)
            features[FeatureKey.TEXT] = text_result["input_ids"]
            features[FeatureKey.TEXT_ATTENTION_MASK] = text_result["attention_mask"]
        
        # å›¾åƒbatchå¤„ç†ï¼ˆåªå¤„ç†ç‹¬ç«‹å›¾åƒï¼Œè§†é¢‘å¸§å·²åœ¨è§†é¢‘å¤„ç†ä¸­å®Œæˆï¼‰
        if any(img is not None for img in modality_data[Modality.IMAGE.value]):
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰16å¸§æ ¼å¼ï¼ˆæ¥è‡ªè§†é¢‘ï¼‰
            has_video_frames = FeatureKey.IMAGE in features and features[FeatureKey.IMAGE].ndim == 5
            
            # åªå¤„ç†ç‹¬ç«‹å›¾åƒï¼ˆä¸æ˜¯16å¸§æ ¼å¼ï¼‰
            valid_images = []
            valid_indices = []
            for idx, img_data in enumerate(modality_data[Modality.IMAGE.value]):
                if img_data is not None:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯16å¸§æ ¼å¼ï¼ˆæ¥è‡ªè§†é¢‘ï¼‰
                    if isinstance(img_data, np.ndarray) and img_data.ndim == 3 and img_data.shape[0] == 16:
                        # è¿™æ˜¯è§†é¢‘å¸§ï¼Œè·³è¿‡ï¼ˆå·²åœ¨è§†é¢‘å¤„ç†ä¸­å¤„ç†ï¼‰
                        continue
                    if isinstance(img_data, str):
                        valid_images.append(Image.open(img_data))
                    elif isinstance(img_data, Image.Image):
                        valid_images.append(img_data)
                    else:
                        # å¯èƒ½æ˜¯å…¶ä»–æ ¼å¼ï¼Œå°è¯•å¤„ç†
                        valid_images.append(img_data)
                    valid_indices.append(idx)
            
            if valid_images:
                # å¤„ç†ç‹¬ç«‹å›¾åƒ
                processed_images = self.image_processor.process_image(valid_images)
                
                if has_video_frames and not inference_mode:
                    # å¦‚æœå·²æœ‰16å¸§æ ¼å¼ï¼Œéœ€è¦å°†ç‹¬ç«‹å›¾åƒè½¬æ¢ä¸º16å¸§æ ¼å¼ï¼ˆå¤åˆ¶ç¬¬ä¸€å¸§ï¼‰
                    for valid_idx, processed_idx in enumerate(valid_indices):
                        # å°†å•å¸§å¤åˆ¶åˆ°16å¸§çš„ç¬¬ä¸€å¸§
                        features[FeatureKey.IMAGE][processed_idx, 0] = processed_images[valid_idx]
                        
                        # ğŸ”§ è¡¥å……ï¼šå¦‚æœä¸æ‰©å±•ï¼ˆå³åªå¡«ç¬¬0å¸§ï¼‰ï¼Œåç»­çš„np.repeaté€»è¾‘éœ€è¦çŸ¥é“è¿™ä¸€ç‚¹ã€‚
                        # ä½†å®é™…ä¸Šï¼Œè¿™é‡Œçš„é€»è¾‘æ˜¯æŠŠå•å¸§æ”¾å…¥5Dæ•°ç»„çš„ç¬¬0å¸§ä½ç½®ã€‚
                        # å¦‚æœä¸æ‰©å±•ï¼Œå…¶ä»–15å¸§æ˜¯0ï¼ˆæˆ–æœªåˆå§‹åŒ–ï¼‰ã€‚
                        # å¦‚æœinference_mode=Trueï¼Œä¸”æˆ‘ä»¬æ··åˆäº†batchï¼ˆä¸æ¨èï¼‰ï¼Œ
                        # é‚£ä¹ˆè¿™é‡Œçš„ features[FeatureKey.IMAGE] æ˜¯5Dçš„ã€‚
                        # æˆ‘ä»¬åªèƒ½å¡«å…¥ç¬¬0å¸§ã€‚
                        # åç»­æ¨¡å‹å¦‚æœåªå–ç¬¬0å¸§ï¼ˆé’ˆå¯¹image sourceï¼‰ï¼Œé‚£å°±æ²¡äº‹ã€‚
                        # ä½†é€šå¸¸æ¨¡å‹ä¼šå¯¹16å¸§åšå¹³å‡ã€‚
                        # æ‰€ä»¥æ··åˆbatchåœ¨inference_mode=Trueä¸‹æ˜¯ä¸å®‰å…¨çš„ï¼Œé™¤éæ¨¡å‹èƒ½è¯†åˆ« paddingã€‚
                        # ä½†è¿™é‡Œæˆ‘ä»¬åªè´Ÿè´£é¢„å¤„ç†ã€‚
                else:
                    # åˆ›å»ºå•å¸§æ ¼å¼çš„batchæ•°ç»„ï¼ˆåªåŒ…å«æœ‰å›¾åƒçš„æ ·æœ¬ï¼‰
                    batch_images = np.stack(processed_images)
                    features[FeatureKey.IMAGE] = batch_images
        
        # è§†é¢‘batchå¤„ç†ï¼šåˆ†è§£ä¸ºå›¾åƒå’ŒéŸ³é¢‘
        if any(video is not None for video in modality_data[Modality.VIDEO.value]):
            video_metadata = {}  # è®°å½•è§†é¢‘æ ·æœ¬ä¿¡æ¯

            for idx, video_data in enumerate(modality_data[Modality.VIDEO.value]):
                if video_data is not None:
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç‹¬ç«‹çš„image/audioæ–‡ä»¶ï¼ˆç”¨äºå†³ç­–æ—¥å¿—ï¼‰
                    has_image_file = (
                        FeatureKey.IMAGE in features and 
                        features[FeatureKey.IMAGE] is not None and
                        (features[FeatureKey.IMAGE].ndim == 4 or 
                         (features[FeatureKey.IMAGE].ndim == 5 and idx < features[FeatureKey.IMAGE].shape[0]))
                    )
                    has_audio_file = (
                        modality_data[Modality.AUDIO.value][idx] is not None
                    )
                    
                    # ä½¿ç”¨å…¬å…±æ–¹æ³•å¤„ç†è§†é¢‘ï¼ˆè¿”å›16å¸§ï¼‰
                    video_features = self._process_video_to_features(video_data)
                    video_frames = video_features[FeatureKey.IMAGE]  # (16, 3, 224, 224)
                    
                    # è·å–è§†é¢‘metadataå’Œsourceä¿¡æ¯
                    video_meta = video_features["_video_metadata"]
                    image_source = video_meta.get("image_source", ModalitySource.VIDEO.value)
                    audio_source = video_meta.get("audio_source")
                    
                    # æ·»åŠ å†³ç­–æ—¥å¿—
                    if image_source == ModalitySource.VIDEO.value and has_image_file:
                        logger.debug(f"æ ·æœ¬{idx}: IMAGEè¢«è§†é¢‘å¸§è¦†ç›–")
                    if audio_source == ModalitySource.VIDEO.value and has_audio_file:
                        logger.debug(f"æ ·æœ¬{idx}: AUDIOè¢«è§†é¢‘éŸ³é¢‘è¦†ç›–")
                    
                    # æ ‡è®°ä¸ºè§†é¢‘æ ·æœ¬ï¼ˆåŒ…å«sourceä¿¡æ¯ï¼‰
                    video_metadata[idx] = video_meta

                    # ä¼˜å…ˆä½¿ç”¨è§†é¢‘æå–çš„å¸§ï¼ˆä¿æŒæ—¶åºå¯¹é½ï¼‰
                    modality_data[Modality.IMAGE.value][idx] = video_frames
                    
                    # æ›´æ–°features[FeatureKey.IMAGE]ï¼Œå°†16å¸§å†™å…¥å¯¹åº”ä½ç½®
                    if FeatureKey.IMAGE in features:
                        # æ£€æŸ¥ç°æœ‰IMAGEç‰¹å¾çš„shape
                        if features[FeatureKey.IMAGE].ndim == 4:
                            if not inference_mode:
                                # ç°æœ‰çš„æ˜¯å•å¸§æ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸º16å¸§æ ¼å¼
                                # å°†ç°æœ‰å•å¸§æ‰©å±•ä¸º16å¸§ï¼ˆå¤åˆ¶ç¬¬ä¸€å¸§ï¼‰
                                existing_frames = features[FeatureKey.IMAGE]
                                expanded_frames = np.repeat(existing_frames[:, np.newaxis, :, :], 16, axis=1)
                                expanded_frames[idx] = video_frames
                                features[FeatureKey.IMAGE] = expanded_frames
                            else:
                                logger.warning("Inference mode: Mixed batch of images and videos detected. "
                                             "Skipping expansion of images to avoid redundancy, but this may cause shape mismatch. "
                                             "Please avoid mixing images and videos in inference mode.")
                                # å°è¯•ç›´æ¥èµ‹å€¼ï¼ˆå¦‚æœå½¢çŠ¶ä¸åŒ¹é…ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
                                try:
                                    features[FeatureKey.IMAGE][idx] = video_frames
                                except ValueError:
                                    # å¦‚æœå¤±è´¥ï¼Œå›é€€åˆ°æ‰©å±•æ¨¡å¼ï¼ˆä¸ºäº†Fail-Safeï¼‰
                                    logger.warning("Shape mismatch, falling back to expansion.")
                                    existing_frames = features[FeatureKey.IMAGE]
                                    expanded_frames = np.repeat(existing_frames[:, np.newaxis, :, :], 16, axis=1)
                                    expanded_frames[idx] = video_frames
                                    features[FeatureKey.IMAGE] = expanded_frames
                        elif features[FeatureKey.IMAGE].ndim == 5:
                            # å·²ç»æ˜¯16å¸§æ ¼å¼ï¼Œç›´æ¥æ›´æ–°
                            features[FeatureKey.IMAGE][idx] = video_frames
                    else:
                        # å¦‚æœIMAGEç‰¹å¾è¿˜ä¸å­˜åœ¨ï¼Œåˆ›å»º16å¸§æ ¼å¼ï¼ˆåªåŒ…å«å½“å‰è§†é¢‘æ ·æœ¬ï¼‰
                        features[FeatureKey.IMAGE] = video_frames[np.newaxis, :]

                    # ä¼˜å…ˆä½¿ç”¨è§†é¢‘ä¸­çš„éŸ³é¢‘ï¼ˆä¿æŒæ—¶åºå¯¹é½ï¼Œä½†validä¼˜å…ˆäºsourceï¼‰
                    video_meta = video_features["_video_metadata"]
                    video_audio_valid = video_meta.get("audio_valid", False)
                    independent_audio_valid = has_audio_file
                    
                    if video_audio_valid and FeatureKey.AUDIO in video_features:
                        # è§†é¢‘éŸ³é¢‘æœ‰æ•ˆï¼Œä½¿ç”¨è§†é¢‘éŸ³é¢‘
                        video_audio = video_features[FeatureKey.AUDIO]
                        modality_data[Modality.AUDIO.value][idx] = video_audio
                    elif independent_audio_valid:
                        # è§†é¢‘éŸ³é¢‘æ— æ•ˆï¼Œä½†ç‹¬ç«‹éŸ³é¢‘æœ‰æ•ˆï¼Œä½¿ç”¨ç‹¬ç«‹éŸ³é¢‘
                        # ç‹¬ç«‹éŸ³é¢‘å·²åœ¨éŸ³é¢‘å¤„ç†é˜¶æ®µå¤„ç†ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–æ“ä½œ
                        pass
                    else:
                        # ä¸¤è€…éƒ½æ— æ•ˆï¼Œaudio maskå°†åœ¨åç»­è®¾ç½®ä¸ºFalse
                        pass
                else:
                    video_metadata[idx] = {"has_video": False}

            # å°†è§†é¢‘å…ƒæ•°æ®æ·»åŠ åˆ°featuresä¸­
            features["_video_metadata"] = video_metadata

        # éŸ³é¢‘batchå¤„ç†
        # ğŸ”§ ä¿®å¤ï¼šç¦»çº¿é¢„å¤„ç†æ¨¡å¼ä¸‹ï¼Œå¦‚æœéŸ³é¢‘æ— æ³•åŠ è½½ï¼ŒæŠ›å‡ºå¼‚å¸¸è·³è¿‡æ ·æœ¬
        # æ³¨æ„ï¼šprocess_batchä¸»è¦ç”¨äºåœ¨çº¿é¢„å¤„ç†ï¼Œç¦»çº¿é¢„å¤„ç†ä½¿ç”¨processå•æ ·æœ¬å¤„ç†
        # ä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œbatchæ¨¡å¼ä¹Ÿåº”è¯¥åœ¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        audio_batch = []
        for audio_data in modality_data[Modality.AUDIO.value]:
            if audio_data is not None:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯å¤„ç†è¿‡çš„numpyæ•°ç»„ï¼ˆæ¥è‡ªè§†é¢‘åˆ†è§£ï¼‰
                if isinstance(audio_data, np.ndarray):
                    # å·²ç»æ˜¯å¤„ç†è¿‡çš„æ¢…å°”é¢‘è°±ï¼Œç›´æ¥ä½¿ç”¨
                    audio_batch.append(audio_data)
                else:
                    # æ˜¯æ–‡ä»¶è·¯å¾„æˆ–BytesIOï¼Œéœ€è¦å¤„ç†
                    # ğŸ”§ ä¿®å¤ï¼šå¦‚æœå¤„ç†å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼ˆç¦»çº¿é¢„å¤„ç†ä¼šè·³è¿‡æ•´ä¸ªæ ·æœ¬ï¼‰
                    audio_result = self.audio_processor.process_audio(audio_data)
                    audio_batch.append(audio_result)
            # ç¼ºå¤±çš„éŸ³é¢‘ä¸æ·»åŠ åˆ°batchä¸­
        
        if audio_batch:
            features[FeatureKey.AUDIO] = np.stack(audio_batch)

        # ç»Ÿä¸€åˆ›å»ºmaskï¼šåŸºäºä¼˜å…ˆçº§è§„åˆ™å¤„ç†åçš„æœ€ç»ˆmodality_dataçŠ¶æ€
        # ä¼˜å…ˆçº§è§„åˆ™å·²å¤„ç†å®Œæˆï¼š
        # - å›¾ç‰‡+è§†é¢‘ï¼šä¼˜å…ˆä½¿ç”¨è§†é¢‘æå–çš„å¸§ï¼ˆ16å¸§ï¼‰
        # - éŸ³é¢‘+è§†é¢‘ï¼šä¼˜å…ˆä½¿ç”¨è§†é¢‘ä¸­çš„éŸ³é¢‘ï¼›å¦‚æœè§†é¢‘æ²¡æœ‰éŸ³é¢‘ï¼Œä½¿ç”¨ç‹¬ç«‹éŸ³é¢‘
        # - æ— è§†é¢‘ï¼šæ­£å¸¸ä½¿ç”¨ç‹¬ç«‹æ•°æ®
        # æ³¨æ„ï¼šVIDEO maskä¸å†éœ€è¦ï¼Œè§†é¢‘å·²åˆ†è§£ä¸ºIMAGEå’ŒAUDIO
        modality_masks = {
            Modality.TEXT.value: np.array(
                [text is not None for text in modality_data[Modality.TEXT.value]], 
                dtype=np.bool_
            ),
            Modality.IMAGE.value: np.array(
                [img is not None for img in modality_data[Modality.IMAGE.value]], 
                dtype=np.bool_
            ),
            Modality.AUDIO.value: np.array(
                [audio is not None for audio in modality_data[Modality.AUDIO.value]], 
                dtype=np.bool_
            ),
        }

        # å°†modality_masksæ·»åŠ åˆ°è¿”å›çš„featuresä¸­
        features["_modality_masks"] = modality_masks
        
        return features

    def _process_video_to_features(self, video_path: str) -> Dict[str, Any]:
        """å¤„ç†è§†é¢‘æ•°æ®ï¼Œæå–å¸§å’ŒéŸ³é¢‘ç‰¹å¾ï¼ˆFail-Safe ç‰ˆæœ¬ï¼‰ã€‚
        
        è¿”å› 16 å¸§ï¼ˆä¸æ˜¯å¤šå¸§å¹³å‡ï¼‰ï¼Œåœ¨ embedding ç©ºé—´åš mean pool ä¿ç•™æ—¶åºä¿¡æ¯ã€‚
        
        Fail-Safe ç­–ç•¥ï¼š
        - è§†é¢‘å¸§æå–å¤±è´¥ -> è¿”å›é›¶å‘é‡å¸§ï¼Œframes_valid=False
        - è§†é¢‘éŸ³é¢‘æå–å¤±è´¥ -> è¿”å›ç©º BytesIOï¼Œaudio_valid=False
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ã€‚
            
        Returns:
            ç‰¹å¾å­—å…¸ï¼ŒåŒ…å«ï¼š
            - FeatureKey.IMAGE: è§†é¢‘å¸§ (16, 3, 224, 224), dtype=float32
            - FeatureKey.AUDIO: éŸ³é¢‘æ¢…å°”é¢‘è°± (64, 256), dtype=float32ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            - "_video_metadata": è§†é¢‘å…ƒæ•°æ®å­—å…¸ï¼ŒåŒ…å« image_source, audio_source, valid æ ‡å¿—ç­‰
        """
        # åˆ†è§£è§†é¢‘ï¼ˆå¤±è´¥ä¼šæŠ›å¼‚å¸¸ï¼‰
        video_frames, frame_mask, audio_stream = self._process_video(video_path)
        video_result = self.image_processor.process_video_frames(video_frames, frame_mask)
        
        # è¿”å›16å¸§ï¼ŒMean Poolåœ¨embeddingç©ºé—´è¿›è¡Œ
        frames = video_result["frames"]  # (16, 3, 224, 224)
        valid_mask = video_result["frame_mask"]  # (16,)
        valid_frames = frames[valid_mask == 1]  # åªå–æœ‰æ•ˆå¸§
        frame_count = len(valid_frames)
        
        # å¦‚æœæœ‰æ•ˆå¸§ä¸è¶³16å¸§ï¼Œç”¨æœ€åä¸€å¸§å¡«å……
        if frame_count < 16:
            pad_frames = np.repeat(valid_frames[-1:] if frame_count > 0 else frames[0:1], 
                                  16 - frame_count, axis=0)
            frames = np.concatenate([valid_frames, pad_frames], axis=0) if frame_count > 0 else pad_frames
        
        # è®°å½•æ•°æ®æ¥æºå’Œæœ‰æ•ˆæ€§
        has_video_audio = audio_stream.getvalue() != b''
        
        # æ„å»ºè¿”å›å­—å…¸
        result = {
            FeatureKey.IMAGE: frames,  # (16, 3, 224, 224)
            "_video_metadata": {
                "has_video": True,
                "has_video_image": True,
                "has_video_audio": has_video_audio,
                "frame_count": frame_count,
                "is_video_frames": True,
                "image_source": ModalitySource.VIDEO.value,
                "audio_source": ModalitySource.VIDEO.value if has_video_audio else None,
                "image_valid": True,
                "audio_valid": has_video_audio
            }
        }
        
        # å¤„ç†éŸ³é¢‘ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if has_video_audio:
            result[FeatureKey.AUDIO] = self.audio_processor.process_audio(audio_stream)
            result["_video_metadata"]["audio_valid"] = True
        
        return result

    def _infer_modality(self, input_path: str) -> Modality:
        """æ¨¡æ€è¯†åˆ«åŒé‡æ ¡éªŒï¼šæ‰©å±•ååˆæ­¥æ¨æ–­ + å†…å®¹æ ¡éªŒã€‚
        
        Args:
            input_path: æ–‡ä»¶è·¯å¾„ã€‚
        
        Returns:
            è¯†åˆ«å‡ºçš„æ¨¡æ€æšä¸¾: Modality.IMAGE, Modality.VIDEO, Modality.AUDIO, Modality.TEXTã€‚
        """
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")

        ext = os.path.splitext(input_path)[-1].lower()
        ext_map = {
            ('.jpg', '.jpeg', '.png', '.bmp', '.tiff'): Modality.IMAGE,
            ('.mp4', '.mov', '.avi', '.mkv', '.wmv', '.MOV'): Modality.VIDEO,
            ('.wav', '.mp3', '.flac', '.aac', '.ogg'): Modality.AUDIO,
            ('.txt', '.md', '.csv', '.json'): Modality.TEXT
        }

        inferred_modality = None
        for exts, modality_enum in ext_map.items():
            if ext in exts:
                inferred_modality = modality_enum
                break

        if inferred_modality is None:
            raise ValueError(f"æ— æ³•æ ¹æ®æ‰©å±•åè¯†åˆ«æ¨¡æ€: {input_path}")

        if inferred_modality == Modality.VIDEO:
            try:
                result = subprocess.run(
                    ['ffprobe', '-v', 'error', '-select_streams', 'v:0',
                     '-show_entries', 'stream=codec_type',
                     '-of', 'default=noprint_wrappers=1:nokey=1', input_path],
                    capture_output=True, text=True, check=True, timeout=10
                )
                if result.stdout.strip() == 'video':
                    return Modality.VIDEO
                else:
                    return Modality.AUDIO
            except (subprocess.CalledProcessError, subprocess.TimeoutExpired):
                return Modality.AUDIO

        elif inferred_modality == Modality.IMAGE:
            try:
                with open(input_path, 'rb') as f:
                    header = f.read(10)
                    if (header.startswith(b'\xff\xd8\xff') or
                        header.startswith(b'\x89PNG\r\n\x1a\n') or
                        header.startswith(b'BM') or
                        header.startswith(b'II*\x00') or
                        header.startswith(b'MM\x00*')):
                        return Modality.IMAGE
                    else:
                        raise ValueError(f"å›¾åƒæ–‡ä»¶å¤´æ ¡éªŒå¤±è´¥: {input_path}")
            except Exception as e:
                raise ValueError(f"æ— æ³•éªŒè¯å›¾åƒæ–‡ä»¶: {str(e)}")

        return inferred_modality

    def _process_video(self, video_path: str, target_frames: int = 16) -> tuple[np.ndarray, np.ndarray, BytesIO]:
        """è§†é¢‘å¤„ç†ï¼šæå–å¸§å’ŒéŸ³é¢‘ã€‚
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ã€‚
            target_frames: ç›®æ ‡å¸§æ•°ï¼Œé»˜è®¤ 16ã€‚
            
        Returns:
            (video_frames, frame_mask, audio_stream) å…ƒç»„ï¼š
            - video_frames: è§†é¢‘å¸§ numpy æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (target_frames, H, W, C)ï¼Œdtype=uint8ã€‚
            - frame_mask: å¸§ maskï¼Œå½¢çŠ¶ä¸º (target_frames,)ï¼Œdtype=int64ï¼Œ1=çœŸå®å¸§ï¼Œ0=è¡¥å…¨å¸§ã€‚
            - audio_stream: éŸ³é¢‘å†…å­˜æµï¼ˆå¦‚æœè§†é¢‘æœ‰éŸ³é¢‘è½¨é“ï¼‰ã€‚
            
        Raises:
            ValueError: å½“è§†é¢‘å¸§æå–å¤±è´¥æ—¶ã€‚
            FileNotFoundError: å½“ ffmpeg æœªæ‰¾åˆ°æ—¶ã€‚
        """
        # æå–å¸§ï¼ˆå¤±è´¥ä¼šæŠ›å¼‚å¸¸ï¼‰
        video_frames = self._extract_frames_from_video(video_path, target_frames=target_frames)
        frame_mask = np.ones(target_frames, dtype=np.int64)
        
        # æå–éŸ³é¢‘ï¼ˆå¦‚æœè§†é¢‘æ²¡æœ‰éŸ³é¢‘è½¨é“ä¼šæŠ›å¼‚å¸¸ï¼Œéœ€è¦æ•è·ï¼‰
        try:
            audio_stream = self._extract_audio_from_video(video_path)
        except ValueError:
            # è§†é¢‘æ²¡æœ‰éŸ³é¢‘è½¨é“æ˜¯æ­£å¸¸çš„ï¼Œè¿”å›ç©ºBytesIO
            audio_stream = BytesIO()
        
        return video_frames, frame_mask, audio_stream

    def _extract_frames_from_video(self, video_path: str, target_frames: int = 16) -> np.ndarray:
        """ä»è§†é¢‘ä¸­æå–æŒ‡å®šæ•°é‡çš„å¸§ã€‚
        
        ä½¿ç”¨ ffmpeg ç›´æ¥åœ¨è§†é¢‘ä¸­å‡åŒ€é‡‡æ · target_frames å¸§ï¼Œä¸æå–æ‰€æœ‰å¸§ã€‚
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ã€‚
            target_frames: ç›®æ ‡å¸§æ•°ï¼Œé»˜è®¤ 16ã€‚
        
        Returns:
            video_frames: è§†é¢‘å¸§ numpy æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (target_frames, H, W, C)ï¼Œdtype=uint8ã€‚
            
        Raises:
            ValueError: å½“è§†é¢‘å¸§æå–å¤±è´¥æ—¶ã€‚
            FileNotFoundError: å½“ ffmpeg æœªæ‰¾åˆ°æ—¶ã€‚
        """
        # å°è¯•ç”¨ffprobeè·å–è§†é¢‘ä¿¡æ¯
        width = None
        height = None
        total_frames_in_video = None
        
        try:
            probe_result = subprocess.run(
                ['ffprobe', '-v', 'error', '-select_streams', 'v:0',
                 '-show_entries', 'stream=width,height,duration,nb_frames',
                 '-of', 'json', video_path],
                capture_output=True, text=True, check=True, timeout=10
            )
            import json
            video_info = json.loads(probe_result.stdout)
            stream = video_info['streams'][0]
            width = stream['width']
            height = stream['height']
            total_frames_in_video = stream.get('nb_frames')
            if total_frames_in_video:
                total_frames_in_video = int(total_frames_in_video)
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError, KeyError, IndexError, json.JSONDecodeError):
            # ffprobeå¤±è´¥ä¸å½±å“ï¼Œç»§ç»­ç”¨ffmpegæå–å¸§
            pass

        # ä½¿ç”¨ffmpegæå–å¸§
        try:
            if total_frames_in_video and total_frames_in_video > 0:
                step = max(1, total_frames_in_video // target_frames)
                vf_filter = f"select='not(mod(n\\,{step}))',scale=256:-1"
            else:
                vf_filter = f"fps={target_frames/10.0:.1f},scale=256:-1"
            
            ffmpeg_cmd = [
                'ffmpeg', '-i', video_path,
                '-f', 'rawvideo', '-pix_fmt', 'rgb24',
                '-vf', vf_filter,
                'pipe:1'
            ]
            result = subprocess.run(ffmpeg_cmd, capture_output=True, check=True, timeout=120)
        except subprocess.TimeoutExpired as e:
            raise ValueError(f"è§†é¢‘å¸§æå–è¶…æ—¶: {video_path}") from e
        except subprocess.CalledProcessError as e:
            error_msg = e.stderr if isinstance(e.stderr, str) else (e.stderr.decode('utf-8', errors='ignore') if e.stderr else "æœªçŸ¥é”™è¯¯")
            error_preview = error_msg.split('\n')[0] if error_msg else "æœªçŸ¥é”™è¯¯"
            raise ValueError(f"ffmpegæå–å¸§å¤±è´¥: {video_path}, é”™è¯¯: {error_preview}") from e
        except FileNotFoundError:
            raise FileNotFoundError(f"ffmpegæœªæ‰¾åˆ°ï¼Œæ— æ³•æå–è§†é¢‘å¸§: {video_path}")

        # è§£æå¸§æ•°æ®
        try:
            frame_data = np.frombuffer(result.stdout, dtype=np.uint8)
            scale_width = 256
            
            if width is not None and height is not None and width > 0:
                scale_height = int(height * scale_width / width)
            else:
                scale_height = 256
            
            # å¦‚æœæ— æ³•è·å–è§†é¢‘å°ºå¯¸ï¼Œä»å®é™…æ•°æ®æ¨æ–­å¸§å°ºå¯¸
            if width is None or height is None:
                possible_heights = [256, 240, 224, 192, 180, 144]
                extracted_frames = 0
                actual_height = None
                
                for test_height in possible_heights:
                    test_frame_size = scale_width * test_height * 3
                    test_frames = len(frame_data) // test_frame_size
                    if test_frames > 0 and len(frame_data) % test_frame_size == 0:
                        extracted_frames = test_frames
                        actual_height = test_height
                        break
                
                if extracted_frames == 0:
                    actual_height = 256
                    frame_size = scale_width * actual_height * 3
                    extracted_frames = len(frame_data) // frame_size
                    if extracted_frames == 0:
                        raise ValueError(f"æ— æ³•æ¨æ–­è§†é¢‘å¸§å°ºå¯¸: {video_path}")
                scale_height = actual_height
            else:
                frame_size = scale_width * scale_height * 3
                extracted_frames = len(frame_data) // frame_size
            
            if extracted_frames == 0:
                raise ValueError(f"æœªæå–åˆ°ä»»ä½•å¸§: {video_path}")
            
            # é‡å¡‘ä¸ºå¸§æ•°ç»„
            frame_size = scale_width * scale_height * 3
            frames = frame_data[:extracted_frames * frame_size].reshape((extracted_frames, scale_height, scale_width, 3)).copy()
            
            # å¦‚æœæå–çš„å¸§æ•°ä¸ç­‰äºç›®æ ‡å¸§æ•°ï¼Œè¿›è¡Œé‡‡æ ·æˆ–å¡«å……
            if extracted_frames > target_frames:
                indices = np.linspace(0, extracted_frames - 1, target_frames, dtype=np.int32)
                frames = frames[indices]
            elif extracted_frames < target_frames:
                pad_frames = np.repeat(frames[-1:], target_frames - extracted_frames, axis=0)
                frames = np.concatenate([frames, pad_frames], axis=0)
            
            return frames[:target_frames]
        except (ValueError, IndexError) as e:
            raise ValueError(f"è§†é¢‘å¸§æ•°æ®è§£æå¤±è´¥: {video_path}, é”™è¯¯: {e}") from e

    def _extract_audio_from_video(self, video_path: str) -> BytesIO:
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘å¹¶ä¿å­˜åˆ°å†…å­˜æµã€‚
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ã€‚
        
        Returns:
            audio_stream: åŒ…å«éŸ³é¢‘æ•°æ®çš„ BytesIO å†…å­˜æµã€‚
            
        Raises:
            ValueError: å½“è§†é¢‘æ²¡æœ‰éŸ³é¢‘è½¨é“æˆ–éŸ³é¢‘æå–å¤±è´¥æ—¶ã€‚
            FileNotFoundError: å½“ ffmpeg æœªæ‰¾åˆ°æ—¶ã€‚
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰éŸ³é¢‘è½¨é“
        probe_result = subprocess.run(
            ['ffprobe', '-v', 'error', '-select_streams', 'a:0',
             '-show_entries', 'stream=codec_type',
             '-of', 'default=noprint_wrappers=1:nokey=1', video_path],
            capture_output=True, text=True, timeout=10
        )
        
        if probe_result.returncode != 0 or probe_result.stdout.strip() != 'audio':
            raise ValueError(f"è§†é¢‘æ²¡æœ‰éŸ³é¢‘è½¨é“: {video_path}")
        
        try:
            result = subprocess.run(
                ['ffmpeg', '-i', video_path,
                 '-vn',
                 '-acodec', 'pcm_s16le',
                 '-ar', '16000',
                 '-ac', '1',
                 '-f', 'wav',
                 'pipe:1'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
                timeout=60
            )

            audio_stream = BytesIO(result.stdout)
            audio_stream.seek(0)
            return audio_stream

        except subprocess.CalledProcessError as e:
            error_msg = e.stderr if isinstance(e.stderr, str) else (e.stderr.decode('utf-8', errors='ignore') if e.stderr else "æœªçŸ¥é”™è¯¯")
            error_preview = error_msg.split('\n')[0] if error_msg else "æœªçŸ¥é”™è¯¯"
            raise ValueError(f"è§†é¢‘éŸ³é¢‘æå–å¤±è´¥: {video_path}, é”™è¯¯: {error_preview}") from e
        except subprocess.TimeoutExpired:
            raise ValueError(f"è§†é¢‘éŸ³é¢‘æå–è¶…æ—¶: {video_path}")
        except FileNotFoundError:
            raise FileNotFoundError(f"ffmpegæœªæ‰¾åˆ°ï¼Œæ— æ³•æå–è§†é¢‘éŸ³é¢‘: {video_path}")


__all__ = ['Preprocessor']
