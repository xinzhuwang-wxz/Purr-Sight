"""
PyTorch Lightning æ¨¡å—

å°è£…å¯¹æ¯”å­¦ä¹ å¯¹é½è®­ç»ƒçš„ LightningModuleï¼Œå®ç°å¤šæ¨¡æ€ç‰¹å¾å¯¹é½çš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚

åŒ…å«ï¼š
- adapt_image_frames: FrameAdapterå‡½æ•°ï¼ˆå·²åºŸå¼ƒï¼Œä»…ç”¨äºå‘åå…¼å®¹ï¼‰
- ContrastiveAlignmentModule: å¯¹æ¯”å­¦ä¹ å¯¹é½è®­ç»ƒçš„Lightningæ¨¡å—

æ•°æ®æµï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰ï¼š
1. Dataset.__getitem__() è¿”å›numpyæ ¼å¼å•æ ·æœ¬ç‰¹å¾ï¼ˆæ— batchç»´åº¦ï¼Œæ— modality_masksï¼‰
2. collate_batch() åˆå¹¶ä¸ºbatchæ ¼å¼numpyæ•°ç»„ï¼Œåˆ›å»ºmodality_masksï¼ˆnumpyæ ¼å¼ï¼‰
3. training_step() åœ¨GPUä¸Šè½¬æ¢ä¸ºtensor
4. encode_batch() è¿›è¡Œbatchç¼–ç ï¼ˆåˆ©ç”¨GPUå¹¶è¡Œï¼Œ16å¸§æ ¼å¼å·²ç»Ÿä¸€ï¼‰
5. forward() å¯¹é½ç‰¹å¾ï¼Œæ¨æ–­modality_presence
6. loss_fn() è®¡ç®—InfoNCEæŸå¤±
"""

import torch
import torch.nn as nn
import pytorch_lightning as pl
import numpy as np
from typing import Dict, Tuple, Any, Optional, List, Union

from purrsight.alignment import ContrastiveAligner, ContrastiveLoss
from purrsight.encoder import _ImageEncoder, _TextEncoder, _AudioEncoder
from purrsight.config import Modality, FeatureKey
from purrsight.utils.eval.eval_align import ContrastiveMetrics
from purrsight.utils.logging import logger
from .train_align_conf import AlignmentConfig


def adapt_image_frames(image_tensor: torch.Tensor) -> Tuple[torch.Tensor, str]:
    """
    é€‚é…å›¾åƒå¸§æ ¼å¼ï¼Œç»Ÿä¸€ä¸º16å¸§æ ¼å¼ï¼ˆFrameAdapterï¼‰
    
    âš ï¸ å·²åºŸå¼ƒï¼šæ­¤å‡½æ•°ä»…ç”¨äºå‘åå…¼å®¹ã€‚
    
    ä¸»è¦é€»è¾‘å·²å†…è”åˆ°encode_batch()ä¸­ã€‚æ ¼å¼è½¬æ¢ç°åœ¨åœ¨collateé˜¶æ®µï¼ˆsmart_catï¼‰å®Œæˆï¼Œ
    æ­¤å‡½æ•°ä»…åœ¨å‘åå…¼å®¹åˆ†æ”¯ä¸­ä½¿ç”¨ï¼ˆå½“æ£€æµ‹åˆ°å•å¸§æ ¼å¼æ—¶ï¼‰ã€‚
    
    Args:
        image_tensor: å›¾åƒtensorï¼Œå¯èƒ½æ˜¯ï¼š
            - (B, 3, H, W): å•å¸§æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            - (B, 16, 3, H, W): 16å¸§æ ¼å¼ï¼ˆæ­£å¸¸æƒ…å†µï¼‰
    
    Returns:
        (adapted_tensor, frame_mode)å…ƒç»„ï¼š
        - adapted_tensor: é€‚é…åçš„tensorï¼Œç»Ÿä¸€ä¸º(B, 16, 3, H, W)
        - frame_mode: å¸§æ¨¡å¼ï¼Œ"single_frame_expanded" æˆ– "video_frames"
    """
    # ğŸ”§ P2ä¿®å¤ï¼šcollateé˜¶æ®µå·²ç»Ÿä¸€æ ¼å¼ï¼Œè¿™é‡ŒåªåšéªŒè¯å’Œå‘åå…¼å®¹å¤„ç†
    if image_tensor.dim() == 5 and image_tensor.shape[1] == 16:
        # å·²ç»æ˜¯16å¸§æ ¼å¼ï¼ˆcollateé˜¶æ®µå·²å¤„ç†ï¼‰
        adapted_tensor = image_tensor
        # æ£€æµ‹æ˜¯å¦ä¸ºå•å¸§æ‰©å±•ï¼šæ£€æŸ¥16å¸§æ˜¯å¦ç›¸åŒ
        frame_mode = "video_frames"  # é»˜è®¤å‡è®¾æ˜¯è§†é¢‘å¸§
        if image_tensor.shape[0] > 0:
            first_frame = image_tensor[0, 0]
            last_frame = image_tensor[0, -1]
            if torch.allclose(first_frame, last_frame, atol=1e-6):
                frame_mode = "single_frame_expanded"
    elif image_tensor.dim() == 4:
        # å•å¸§æ ¼å¼ï¼ˆä¸åº”è¯¥å‡ºç°ï¼Œcollateé˜¶æ®µåº”å·²è½¬æ¢ï¼‰
        # ä½†ä¸ºäº†å‘åå…¼å®¹ï¼Œä»ç„¶å¤„ç†
        adapted_tensor = image_tensor.unsqueeze(1).expand(-1, 16, -1, -1, -1)
        frame_mode = "single_frame_expanded"
    else:
        # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨
        adapted_tensor = image_tensor
        frame_mode = "unknown"
    
    return adapted_tensor, frame_mode


class ContrastiveAlignmentModule(pl.LightningModule):
    """
    å¯¹æ¯”å­¦ä¹ å¯¹é½çš„ LightningModule
    
    åœ¨ batch çº§åˆ«è¿›è¡Œç¼–ç ï¼Œåˆ©ç”¨ GPU å¹¶è¡Œèƒ½åŠ›ã€‚
    
    Attributes:
        config: è®­ç»ƒé…ç½®
        image_encoder: å›¾åƒç¼–ç å™¨ï¼ˆå†»ç»“ï¼‰
        text_encoder: æ–‡æœ¬ç¼–ç å™¨ï¼ˆå†»ç»“ï¼‰
        audio_encoder: éŸ³é¢‘ç¼–ç å™¨ï¼ˆå†»ç»“ï¼‰
        aligner: å¯¹æ¯”å­¦ä¹ å¯¹é½å™¨
        loss_fn: å¯¹æ¯”æŸå¤±å‡½æ•°
        metrics: è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨
    """

    def __init__(self, config: AlignmentConfig):
        """
        åˆå§‹åŒ– LightningModule

        Args:
            config: è®­ç»ƒé…ç½®
        """
        super().__init__()

        # ä¿å­˜é…ç½®
        self.config = config

        # åˆå§‹åŒ–ç¼–ç å™¨ï¼ˆå†»ç»“ï¼Œevalæ¨¡å¼ï¼‰
        # ç¼–ç å™¨åœ¨ LightningModule ä¸­åˆå§‹åŒ–ä¸€æ¬¡ï¼Œæ‰€æœ‰ batch å…±äº«
        self._init_encoders()

        # è·å–ç¼–ç å™¨å®é™…è¾“å‡ºç»´åº¦
        text_dim = 384  # MiniLMå›ºå®šè¾“å‡º
        image_dim = self.image_encoder.feature_dim  # ä»ImageEncoderè·å–
        audio_dim = 2048  # Cnn14å›ºå®šè¾“å‡º

        # åˆå§‹åŒ–å¯¹é½å™¨ï¼ˆä¼ å…¥å„æ¨¡æ€çš„å®é™…ç»´åº¦ï¼‰
        self.aligner = ContrastiveAligner(
            text_input_dim=text_dim,
            image_input_dim=image_dim,
            audio_input_dim=audio_dim,
            output_dim=config.output_dim,
            use_temperature_scaling=config.use_temperature_scaling,
        )

        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.loss_fn = ContrastiveLoss(
            use_temperature_scaling=config.use_temperature_scaling
        )

        # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨
        self.metrics = ContrastiveMetrics(k_values=[1, 5, 10])

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜å¸§æƒé‡ï¼Œé¿å…é‡å¤è®¡ç®—
        self._single_frame_weights = None
        self._video_frame_weights = None
        
        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šdtypeæ£€æŸ¥æ ‡å¿—ï¼Œåªåœ¨ç¬¬ä¸€æ¬¡æˆ–å‡ºç°é—®é¢˜æ—¶æ£€æŸ¥
        self._dtype_check_needed = True  # ç¬¬ä¸€æ¬¡batchéœ€è¦æ£€æŸ¥
        
        # ğŸ”§ ä¼˜åŒ–ï¼šéªŒè¯æŒ‡æ ‡è®¡ç®—æ§åˆ¶
        self.compute_val_metrics = config.compute_val_metrics
        self.val_metrics_every_n_epochs = config.val_metrics_every_n_epochs

        # ä¿å­˜è¶…å‚æ•°ï¼ˆè‡ªåŠ¨è®°å½•åˆ° MLflowï¼‰
        self.save_hyperparameters(config.__dict__)

    def _init_encoders(self):
        """
        åˆå§‹åŒ–å¹¶å†»ç»“ç¼–ç å™¨
        
        ç¼–ç å™¨ä¼šè‡ªåŠ¨ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ï¼ˆGPU/CPUï¼‰ï¼Œå¹¶è®¾ç½®ä¸ºevalæ¨¡å¼ã€‚
        æ‰€æœ‰ç¼–ç å™¨å‚æ•°è¢«å†»ç»“ï¼Œä¸å‚ä¸æ¢¯åº¦æ›´æ–°ã€‚
        """
        # ç¼–ç å™¨ä¼šè‡ªåŠ¨ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ï¼ˆGPU/CPUï¼‰
        self.image_encoder = _ImageEncoder().eval()
        self.text_encoder = _TextEncoder().eval()
        self.audio_encoder = _AudioEncoder().eval()

        # å†»ç»“ç¼–ç å™¨å‚æ•°
        for encoder in [self.image_encoder, self.text_encoder, self.audio_encoder]:
            for param in encoder.parameters():
                param.requires_grad = False

    def encode_batch(
        self,
        tensor_features: Dict[str, torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """
        Batch çº§åˆ«ç¼–ç ï¼šå°†é¢„å¤„ç†åçš„ tensor_features ç¼–ç ä¸º encoder_outputs
        
        ä½¿ç”¨FrameAdapterç»Ÿä¸€å¤„ç†å•å¸§å›¾åƒå’Œ16å¸§è§†é¢‘ï¼Œç¡®ä¿æ‰€æœ‰å›¾åƒè¾“å…¥åˆ°ç¼–ç å™¨éƒ½æ˜¯16å¸§æ ¼å¼ã€‚
        
        Args:
            tensor_features: é¢„å¤„ç†åçš„ç‰¹å¾å­—å…¸ï¼Œé”®ä¸ºFeatureKeyï¼Œå€¼ä¸ºå½¢çŠ¶ä¸º(B, ...)çš„tensor
                æ•°æ®åœ¨CPUä¸Šï¼Œä¼šåœ¨å‡½æ•°å†…éƒ¨æ‰¹é‡ä¼ è¾“åˆ°GPU/MPS

        Returns:
            encoder_outputs: ç¼–ç å™¨è¾“å‡ºå­—å…¸ï¼Œé”®ä¸ºæ¨¡æ€åç§°ï¼Œå€¼ä¸ºä¸åŒå½¢çŠ¶çš„tensor
                Text: (B, 384), Image: (B, feature_dim), Audio: (B, 2048)
            
        æ³¨æ„ï¼šè§†é¢‘å·²åˆ†è§£ä¸ºImageå’ŒAudioï¼Œä¸å†æœ‰ç‹¬ç«‹çš„Videoç¼–ç è¾“å‡ºã€‚
        """
        encoder_outputs = {}
        device = self.device  # LightningModule çš„è®¾å¤‡

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡ä¼ è¾“æ‰€æœ‰tensoråˆ°deviceï¼Œæ£€æŸ¥è®¾å¤‡é¿å…é‡å¤ä¼ è¾“
        # è·³è¿‡étensorç±»å‹ï¼ˆå¦‚_video_metadataå­—å…¸ï¼‰
        tensor_features_gpu = {}
        for key, value in tensor_features.items():
            if value is not None:
                if isinstance(value, torch.Tensor):
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ£€æŸ¥è®¾å¤‡ï¼Œé¿å…é‡å¤ä¼ è¾“
                    if value.device != device:
                        tensor_features_gpu[key] = value.to(device)
                    else:
                        tensor_features_gpu[key] = value  # å·²åœ¨ç›®æ ‡è®¾å¤‡ï¼Œè·³è¿‡ä¼ è¾“
                else:
                    # étensorç±»å‹ï¼ˆå¦‚_video_metadataå­—å…¸ï¼‰ç›´æ¥ä¿ç•™ï¼Œä¸ä¼ è¾“åˆ°GPU
                    tensor_features_gpu[key] = value
            else:
                tensor_features_gpu[key] = None

        # Batch ç¼–ç ï¼ˆåˆ©ç”¨ GPU å¹¶è¡Œèƒ½åŠ›ï¼‰
        with torch.no_grad():
            # æ–‡æœ¬ç¼–ç 
            if FeatureKey.TEXT in tensor_features_gpu:
                text_input = tensor_features_gpu[FeatureKey.TEXT]
                text_mask = tensor_features_gpu.get(FeatureKey.TEXT_ATTENTION_MASK)
                encoder_outputs[Modality.TEXT.value] = self.text_encoder(text_input, text_mask)

            # å›¾åƒç¼–ç 
            # ğŸ”§ P2ä¿®å¤ï¼šæ ¼å¼è½¬æ¢å·²åœ¨collateé˜¶æ®µï¼ˆsmart_catï¼‰å®Œæˆï¼Œè¿™é‡Œç›´æ¥å¤„ç†16å¸§æ ¼å¼
            if FeatureKey.IMAGE in tensor_features_gpu:
                image_input = tensor_features_gpu[FeatureKey.IMAGE]
                
                # ğŸ”§ P2ä¿®å¤ï¼šcollateé˜¶æ®µå·²ç»Ÿä¸€ä¸º16å¸§æ ¼å¼ï¼Œè¿™é‡Œç›´æ¥å¤„ç†
                # éªŒè¯è¾“å…¥æ ¼å¼ï¼ˆåº”è¯¥æ˜¯16å¸§æ ¼å¼(B, 16, 3, 224, 224)ï¼‰
                if image_input.dim() == 5 and image_input.shape[1] == 16:
                    # 16å¸§æ ¼å¼ï¼ˆåŒ…æ‹¬å•å¸§æ‰©å±•å’Œè§†é¢‘å¸§ï¼‰
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šç®€åŒ–frame_modeæ£€æµ‹ï¼Œåªåœ¨batch_size > 0æ—¶æ£€æµ‹
                    # ä½¿ç”¨æ›´å¿«çš„æ¯”è¾ƒæ–¹æ³•ï¼ˆæ¯”è¾ƒsumè€Œä¸æ˜¯allcloseï¼‰
                    frame_mode = "video_frames"  # é»˜è®¤å‡è®¾æ˜¯è§†é¢‘å¸§
                    if image_input.shape[0] > 0:
                        # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€å¸§å’Œæœ€åä¸€å¸§æ˜¯å¦ç›¸åŒï¼ˆå•å¸§æ‰©å±•çš„ç‰¹å¾ï¼‰
                        # ä½¿ç”¨sumæ¯”è¾ƒæ¯”allcloseæ›´å¿«ï¼Œä¸”å¯¹äºå•å¸§æ‰©å±•ï¼ˆå®Œå…¨ç›¸åŒï¼‰è¶³å¤Ÿå‡†ç¡®
                        first_frame = image_input[0, 0]
                        last_frame = image_input[0, -1]
                        # æ¯”è¾ƒsumæ¯”allcloseå¿«ï¼Œä¸”å¯¹äºå®Œå…¨ç›¸åŒçš„æƒ…å†µè¶³å¤Ÿå‡†ç¡®
                        if torch.equal(first_frame, last_frame) or (first_frame.sum() - last_frame.sum()).abs() < 1e-4:
                            frame_mode = "single_frame_expanded"
                    
                    B, T, C, H, W = image_input.shape
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨reshapeè€Œä¸æ˜¯viewï¼ˆå› ä¸ºexpandåˆ›å»ºçš„æ˜¯éè¿ç»­è§†å›¾ï¼‰
                    # Reshapeä¸º(B*T, C, H, W)
                    image_input_flat = image_input.reshape(B * T, C, H, W)
                    # ç¼–ç ï¼šè¾“å‡º(B*T, feature_dim)
                    encoded_frames = self.image_encoder(image_input_flat)
                    # Reshapeå›(B, T, feature_dim)
                    encoded_frames = encoded_frames.reshape(B, T, -1)
                    
                    # ğŸ”§ P1ä¿®å¤ï¼šå•å¸§å›¾åƒä½¿ç”¨å‡åŒ€æƒé‡ï¼Œ16å¸§è§†é¢‘ä½¿ç”¨çº¿æ€§é€’å¢æƒé‡
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜æƒé‡ï¼Œé¿å…é‡å¤è®¡ç®—
                    if frame_mode == "single_frame_expanded":
                        # å•å¸§å›¾åƒï¼šä½¿ç”¨å‡åŒ€æƒé‡ï¼ˆæ•°å­¦ç­‰ä»·ï¼šmean([x, x, ..., x]) = xï¼‰
                        if self._single_frame_weights is None or self._single_frame_weights.shape[0] != T:
                            self._single_frame_weights = torch.ones(T, device=encoded_frames.device) / T
                        weights = self._single_frame_weights
                    else:
                        # 16å¸§è§†é¢‘ï¼šä½¿ç”¨çº¿æ€§é€’å¢æƒé‡ï¼ˆåé¢çš„å¸§æƒé‡æ›´é«˜ï¼‰ï¼Œä¿ç•™æ—¶åºä¿¡æ¯
                        # æƒé‡èŒƒå›´ [0.5, 1.0]ï¼Œå½’ä¸€åŒ–åæ±‚å’Œ
                        if self._video_frame_weights is None or self._video_frame_weights.shape[0] != T:
                            weights_raw = torch.linspace(0.5, 1.0, T, device=encoded_frames.device)
                            self._video_frame_weights = weights_raw / weights_raw.sum()  # å½’ä¸€åŒ–
                        weights = self._video_frame_weights
                    
                    encoder_outputs[Modality.IMAGE.value] = (encoded_frames * weights.view(1, T, 1)).sum(dim=1)
                elif image_input.dim() == 4:
                    # ğŸ”§ ä¿®å¤ï¼šå•å¸§æ ¼å¼æ˜¯æ­£å¸¸çš„ï¼ˆå¦‚æœbatchéƒ½æ˜¯å•å¸§ï¼Œcollateé˜¶æ®µä¼šä¿æŒå•å¸§æ ¼å¼ï¼‰
                    # ä½¿ç”¨adapt_image_framesè¿›è¡Œè½¬æ¢
                    adapted_image, frame_mode = adapt_image_frames(image_input)
                    if adapted_image.dim() == 5 and adapted_image.shape[1] == 16:
                        B, T, C, H, W = adapted_image.shape
                        image_input_flat = adapted_image.reshape(B * T, C, H, W)
                        encoded_frames = self.image_encoder(image_input_flat)
                        encoded_frames = encoded_frames.reshape(B, T, -1)
                        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜çš„æƒé‡
                        if frame_mode == "single_frame_expanded":
                            if self._single_frame_weights is None or self._single_frame_weights.shape[0] != T:
                                self._single_frame_weights = torch.ones(T, device=encoded_frames.device) / T
                            weights = self._single_frame_weights
                        else:
                            if self._video_frame_weights is None or self._video_frame_weights.shape[0] != T:
                                weights_raw = torch.linspace(0.5, 1.0, T, device=encoded_frames.device)
                                self._video_frame_weights = weights_raw / weights_raw.sum()
                            weights = self._video_frame_weights
                        encoder_outputs[Modality.IMAGE.value] = (encoded_frames * weights.view(1, T, 1)).sum(dim=1)
                    else:
                        encoder_outputs[Modality.IMAGE.value] = self.image_encoder(adapted_image)
                else:
                    # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•ç›´æ¥ç¼–ç 
                    logger.warning(f"æœªçŸ¥çš„å›¾åƒè¾“å…¥æ ¼å¼: shape={image_input.shape}, dim={image_input.dim()}")
                    encoder_outputs[Modality.IMAGE.value] = self.image_encoder(image_input)

            # éŸ³é¢‘ç¼–ç 
            if FeatureKey.AUDIO in tensor_features_gpu:
                audio_input = tensor_features_gpu[FeatureKey.AUDIO]
                encoder_outputs[Modality.AUDIO.value] = self.audio_encoder(audio_input)

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æ¨¡æ€çš„batch sizeä¸€è‡´ï¼ˆä¸åº”è¯¥ä¸ä¸€è‡´ï¼‰
        batch_size = None
        for tensor in encoder_outputs.values():
            if tensor is not None:
                batch_size = tensor.shape[0]
                break

        if batch_size is None:
            raise ValueError("Batch ä¸­æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ¨¡æ€")

        # ğŸ”§ ä¿®å¤ï¼šéªŒè¯ä¸€è‡´æ€§ï¼Œå¦‚æœä¸ä¸€è‡´ç›´æ¥æŠ¥é”™ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
        for modality_key, tensor in encoder_outputs.items():
            if tensor is not None and tensor.shape[0] != batch_size:
                raise ValueError(
                    f"ç¼–ç å™¨è¾“å‡ºbatch sizeä¸ä¸€è‡´ï¼ˆè¿™æ˜¯bugï¼Œä¸åº”è¯¥å‘ç”Ÿï¼‰: "
                    f"{modality_key} shape={tensor.shape}, expected batch_size={batch_size}. "
                    f"è¯·æ£€æŸ¥é¢„å¤„ç†å’Œç¼–ç é€»è¾‘ã€‚å¯èƒ½çš„åŸå› ï¼š"
                    f"1. é¢„å¤„ç†é˜¶æ®µbatch sizeä¸ä¸€è‡´"
                    f"2. ç¼–ç å™¨å¤„ç†äº†ä¸åŒæ•°é‡çš„æ ·æœ¬"
                )

        # ä¸ºç¼ºå¤±çš„æ¨¡æ€åˆ›å»ºé›¶å‘é‡ï¼ˆæ³¨æ„ï¼šVIDEOå·²åˆ†è§£ï¼Œä¸å†éœ€è¦ï¼‰
        for modality in [Modality.TEXT, Modality.IMAGE, Modality.AUDIO]:
            if modality.value not in encoder_outputs:
                # æ ¹æ®æ¨¡æ€ç¡®å®šæ­£ç¡®çš„ç»´åº¦
                if modality == Modality.TEXT:
                    dim = 384
                elif modality == Modality.IMAGE:
                    dim = self.image_encoder.feature_dim
                elif modality == Modality.AUDIO:
                    dim = 2048
                else:
                    dim = 512  # fallback
                
                encoder_outputs[modality.value] = torch.zeros(
                    batch_size, dim, device=device, dtype=torch.float32
                )

        return encoder_outputs

    def forward(
        self,
        encoder_outputs: Dict[str, torch.Tensor],
        modality_masks: Optional[Dict[str, torch.Tensor]] = None
    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, bool]]:
        """
        å‰å‘ä¼ æ’­ï¼šå¯¹é½ç‰¹å¾åˆ°ç»Ÿä¸€è¯­ä¹‰ç©ºé—´

        Args:
            encoder_outputs: ç¼–ç å™¨è¾“å‡ºå­—å…¸ï¼Œé”®ä¸ºæ¨¡æ€åç§°ï¼Œå€¼ä¸ºä¸åŒå½¢çŠ¶çš„tensor
            modality_masks: æ¨¡æ€maskå­—å…¸ï¼Œå¯é€‰ï¼ˆsample-level maskï¼Œå½¢çŠ¶ä¸º(B,)ï¼‰

        Returns:
            (aligned_features, modality_presence)å…ƒç»„ï¼š
            - aligned_features: å¯¹é½åçš„ç‰¹å¾å­—å…¸ï¼Œé”®ä¸ºæ¨¡æ€åç§°ï¼Œå€¼ä¸ºå½¢çŠ¶ä¸º(B, 512)çš„tensor
            - modality_presence: batch-levelæ¨¡æ€å­˜åœ¨æ ‡è®°å­—å…¸ï¼Œç”¨äºä¼ é€’ç»™loss_fn
        """
        # ğŸ”§ ä¼˜åŒ–ï¼šç»Ÿä¸€åœ¨forwardä¸­æ¨æ–­modality_presenceï¼Œé¿å…åœ¨loss_fnä¸­é‡å¤æ¨æ–­
        if modality_masks is not None:
            modality_presence = {
                k: bool(v.any().item()) for k, v in modality_masks.items()
            }
        else:
            # å¦‚æœæ²¡æœ‰maskï¼Œæ ¹æ®encoder_outputsæ¨æ–­
            modality_presence = {
                k: v is not None and not torch.all(v == 0)
                for k, v in encoder_outputs.items()
            }

        aligned_features = self.aligner(encoder_outputs, modality_presence)
        return aligned_features, modality_presence

    def training_step(
        self,
        batch: Tuple[Dict[str, Union[np.ndarray, torch.Tensor]], Dict[str, Union[np.ndarray, torch.Tensor]], int],
        batch_idx: int
    ) -> torch.Tensor:
        """
        è®­ç»ƒæ­¥éª¤

        Args:
            batch: (batch_features, modality_masks, batch_size)å…ƒç»„
                batch_features: é¢„å¤„ç†åçš„ç‰¹å¾å­—å…¸ï¼ˆnumpyæˆ–tensoræ ¼å¼ï¼‰ï¼Œé”®ä¸ºFeatureKey
                modality_masks: æ¨¡æ€maskå­—å…¸ï¼ˆnumpyæˆ–tensoræ ¼å¼ï¼‰ï¼Œé”®ä¸ºæ¨¡æ€åç§°
                batch_size: batchå¤§å°ï¼ˆç”±collate_batchæä¾›ï¼Œé¿å…é‡å¤æ¨æ–­ï¼‰
            batch_idx: batchç´¢å¼•

        Returns:
            æŸå¤±å€¼
        """
        # 1. è®°å½•GPUå†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆåªåœ¨ rank 0 è®°å½•ï¼‰
        if self.trainer.is_global_zero and batch_idx % 50 == 0:
             # GPU Memory
             if torch.cuda.is_available():
                 max_memory = torch.cuda.max_memory_allocated() / 1024**3
                 current_memory = torch.cuda.memory_allocated() / 1024**3
                 self.log("gpu_memory_max_gb", max_memory, prog_bar=False)
                 self.log("gpu_memory_current_gb", current_memory, prog_bar=False)

        batch_features, modality_masks, batch_size = batch

        # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šåœ¨GPUä¸Šè½¬æ¢ä¸ºtensorï¼Œé¿å…åœ¨CPUä¸Šå ç”¨å†…å­˜
        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡è½¬æ¢ï¼Œå‡å°‘å¤šæ¬¡.to(device)è°ƒç”¨
        device = self.device
        
        # è½¬æ¢batch_featuresä¸ºGPU tensor
        tensor_features = {}
        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šå…ˆæ”¶é›†æ‰€æœ‰éœ€è¦è½¬æ¢çš„numpyæ•°ç»„ï¼Œç„¶åæ‰¹é‡è½¬æ¢
        numpy_items = []
        for key, feat in batch_features.items():
            if key == "_video_metadata" or key == "_modality_sources":
                tensor_features[key] = feat  # ä¿ç•™metadataå­—å…¸
                continue
            
            if feat is not None:
                if isinstance(feat, np.ndarray):
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨torch.as_tensorè¿›è¡Œé›¶æ‹·è´è½¬æ¢ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰éæ•´æ•°ç±»å‹éƒ½è½¬æ¢ä¸ºfloat32ï¼ˆmixed precisionè®­ç»ƒéœ€è¦ï¼‰
                    if feat.dtype == np.int64:
                        tensor_feat = torch.from_numpy(feat).long()
                    elif feat.dtype == np.int32:
                        tensor_feat = torch.from_numpy(feat).int()
                    else:
                        # å¤„ç†int8, uint8, int16, uint16, float16ç­‰ç±»å‹ï¼Œç»Ÿä¸€è½¬æ¢ä¸ºfloat32
                        if feat.dtype in [np.int8, np.uint8, np.int16, np.uint16, np.float16]:
                            tensor_feat = torch.from_numpy(feat.astype(np.float32)).float()
                        elif feat.dtype == np.float32:
                            tensor_feat = torch.as_tensor(feat).float()
                        else:
                            tensor_feat = torch.from_numpy(feat.astype(np.float32)).float()
                    numpy_items.append((key, tensor_feat))
                elif isinstance(feat, torch.Tensor):
                    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿tensorçš„dtypeæ­£ç¡®ï¼ˆmixed precisionè®­ç»ƒéœ€è¦float32/int64/int32/boolï¼‰
                    if feat.dtype not in [torch.float32, torch.int64, torch.int32, torch.bool]:
                        # å¦‚æœä¸æ˜¯æ ‡å‡†ç±»å‹ï¼Œè½¬æ¢ä¸ºfloat32
                        if feat.dtype in [torch.int8, torch.uint8, torch.int16, torch.uint16, torch.float16]:
                            feat = feat.float()
                        else:
                            feat = feat.float()
                    numpy_items.append((key, feat))
                else:
                    tensor_features[key] = feat
        
        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡ä¼ è¾“åˆ°GPUï¼Œä½¿ç”¨å¼‚æ­¥ä¼ è¾“ï¼ˆCUDAï¼‰æˆ–åŒæ­¥ä¼ è¾“ï¼ˆMPS/CPUï¼‰
        # ğŸ”§ ä¿®å¤ï¼šdeviceæ˜¯torch.deviceå¯¹è±¡ï¼Œä½¿ç”¨device.typeè€Œä¸æ˜¯startswith
        if device.type == "cuda":
            # CUDAè®¾å¤‡ï¼šä½¿ç”¨non_blockingå¼‚æ­¥ä¼ è¾“ï¼Œæé«˜æ•ˆç‡
            for key, tensor_feat in numpy_items:
                tensor_features[key] = tensor_feat.to(device, non_blocking=True)
        else:
            # MPSæˆ–CPUè®¾å¤‡ï¼šåŒæ­¥ä¼ è¾“
            for key, tensor_feat in numpy_items:
                tensor_features[key] = tensor_feat.to(device)
        
        # è½¬æ¢modality_masksä¸ºGPU tensor
        modality_masks_gpu = {}
        mask_items = []
        for k, v in modality_masks.items():
            if isinstance(v, np.ndarray):
                mask_items.append((k, torch.from_numpy(v).bool()))
            elif isinstance(v, torch.Tensor):
                # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿mask tensoræ˜¯boolç±»å‹
                if v.dtype != torch.bool:
                    v = v.bool()
                mask_items.append((k, v))
            else:
                modality_masks_gpu[k] = v
        
        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡ä¼ è¾“masksåˆ°GPUï¼Œä½¿ç”¨å¼‚æ­¥ä¼ è¾“ï¼ˆCUDAï¼‰
        # ğŸ”§ ä¿®å¤ï¼šdeviceæ˜¯torch.deviceå¯¹è±¡ï¼Œä½¿ç”¨device.typeè€Œä¸æ˜¯startswith
        if device.type == "cuda":
            for k, v in mask_items:
                modality_masks_gpu[k] = v.to(device, non_blocking=True)
        else:
            for k, v in mask_items:
                modality_masks_gpu[k] = v.to(device)

        # æ£€æŸ¥batchæ˜¯å¦ä¸ºç©º
        if not tensor_features:
            # ğŸ”§ æ”¹è¿›ï¼šæ”¹ä¸ºdebugçº§åˆ«ï¼Œå› ä¸ºè¿™æ˜¯é¢„æœŸçš„è¾¹ç•Œæƒ…å†µï¼ˆæ‰€æœ‰æ ·æœ¬mask=Falseæ—¶ï¼‰
            logger.debug(f"è®­ç»ƒæ­¥éª¤ {batch_idx}: tensor_features ä¸ºç©ºï¼ˆå¯èƒ½æ˜¯æ‰€æœ‰æ ·æœ¬mask=Falseï¼‰")
            return torch.tensor(0.0, device=self.device, dtype=torch.float32, requires_grad=True)

        # Batch ç¼–ç ï¼ˆåˆ©ç”¨ GPU å¹¶è¡Œèƒ½åŠ›ï¼‰
        try:
            encoder_outputs = self.encode_batch(tensor_features)
        except Exception as e:
            logger.error(f"è®­ç»ƒæ­¥éª¤ {batch_idx}: ç¼–ç å¤±è´¥ - {e}", exc_info=True)
            raise

        # å¯¹é½ç‰¹å¾
        try:
            aligned_features, modality_presence = self.forward(encoder_outputs, modality_masks_gpu)
        except Exception as e:
            logger.error(f"è®­ç»ƒæ­¥éª¤ {batch_idx}: å¯¹é½å¤±è´¥ - {e}", exc_info=True)
            raise

        # è®¡ç®—æŸå¤±
        try:
            # ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨forwardè¿”å›çš„modality_presenceï¼Œé¿å…é‡å¤æ¨æ–­
            total_loss, loss_dict = self.loss_fn(
                aligned_features,
                modality_presence=modality_presence,  # ä½¿ç”¨forwardæ¨æ–­çš„ç»“æœ
                logit_scales=self.aligner.get_logit_scales() if self.aligner.use_temperature_scaling else None,
                modality_masks=modality_masks_gpu,  # Sample-level maskï¼Œç”¨äºç²¾ç¡®æ§åˆ¶æŸå¤±è®¡ç®—
            )
        except Exception as e:
            logger.error(f"è®­ç»ƒæ­¥éª¤ {batch_idx}: æŸå¤±è®¡ç®—å¤±è´¥ - {e}", exc_info=True)
            raise

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ¡ä»¶dtypeæ£€æŸ¥ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æˆ–å‡ºç°é—®é¢˜æ—¶æ£€æŸ¥ï¼‰
        if self._dtype_check_needed or total_loss.dtype != torch.float32:
            if total_loss.dtype != torch.float32:
                logger.warning(f"è®­ç»ƒæ­¥éª¤ {batch_idx}: æŸå¤±dtypeä¸æ­£ç¡® ({total_loss.dtype})ï¼Œè½¬æ¢ä¸ºfloat32")
                total_loss = total_loss.float()
                self._dtype_check_needed = True  # å‡ºç°é—®é¢˜ï¼Œç»§ç»­æ£€æŸ¥
            else:
                self._dtype_check_needed = False  # æ­£å¸¸ï¼Œåç»­è·³è¿‡æ£€æŸ¥
        
        # ç¡®ä¿æŸå¤±æœ‰æ•ˆä¸”éœ€è¦æ¢¯åº¦
        if not total_loss.requires_grad:
            logger.warning(f"è®­ç»ƒæ­¥éª¤ {batch_idx}: æŸå¤±ä¸éœ€è¦æ¢¯åº¦ï¼Œå¯èƒ½æœ‰é—®é¢˜")
            # ç¡®ä¿æŸå¤±éœ€è¦æ¢¯åº¦
            total_loss = total_loss.detach().requires_grad_(True)
        
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            logger.warning(f"è®­ç»ƒæ­¥éª¤ {batch_idx}: æŸå¤±ä¸º NaN æˆ– Infï¼Œä½¿ç”¨é›¶æŸå¤±")
            total_loss = torch.tensor(0.0, device=total_loss.device, dtype=torch.float32, requires_grad=True)

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šbatch_sizeå·²ä»collate_batchä¼ å…¥ï¼Œæ— éœ€é‡å¤æ¨æ–­
        # batch_sizeå·²åœ¨å‡½æ•°å¼€å§‹æ—¶ä»batchå…ƒç»„ä¸­è§£åŒ…

        # ä»…é€šè¿‡ Lightning çš„ MLflowLogger è®°å½•ï¼ˆç»Ÿä¸€ç”¨ MLflowï¼‰
        self.log("train_loss", total_loss, prog_bar=True, logger=True, on_step=True, on_epoch=True, batch_size=batch_size)
        for loss_name, loss_value in loss_dict.items():
            self.log(f"train_{loss_name}", loss_value, prog_bar=False, logger=True, on_step=False, on_epoch=True, batch_size=batch_size)
        if self.trainer is not None and self.trainer.optimizers:
            lr = self.trainer.optimizers[0].param_groups[0].get("lr")
            if lr is not None:
                self.log("learning_rate", float(lr), prog_bar=False, logger=True, on_step=True, on_epoch=False)

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä¸åœ¨æ¯ä¸ªbatchè®¡ç®—æŒ‡æ ‡ï¼Œæ”¹ä¸ºåœ¨epochç»“æŸæ—¶è®¡ç®—
        # æŒ‡æ ‡è®¡ç®—ç§»åˆ°on_train_epoch_endä¸­ï¼Œé¿å…æ¯ä¸ªbatchéƒ½è®¡ç®—ï¼ˆO(BÂ²)å¤æ‚åº¦ï¼‰

        # ğŸ”§ æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿è¿”å›çš„æ˜¯float32 tensorï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æˆ–å‡ºç°é—®é¢˜æ—¶æ£€æŸ¥ï¼‰
        if batch_idx == 0 or self._dtype_check_needed:
            if not isinstance(total_loss, torch.Tensor):
                raise TypeError(f"è®­ç»ƒæ­¥éª¤ {batch_idx}: lossä¸æ˜¯tensorï¼Œè€Œæ˜¯{type(total_loss)}")
            if total_loss.dtype != torch.float32:
                logger.error(f"è®­ç»ƒæ­¥éª¤ {batch_idx}: loss dtypeä¸æ˜¯float32ï¼Œè€Œæ˜¯{total_loss.dtype}ï¼Œå¼ºåˆ¶è½¬æ¢")
                total_loss = total_loss.float()
                self._dtype_check_needed = True
            if not total_loss.requires_grad:
                logger.warning(f"è®­ç»ƒæ­¥éª¤ {batch_idx}: lossä¸éœ€è¦æ¢¯åº¦ï¼Œå¼ºåˆ¶è®¾ç½®")
                total_loss = total_loss.detach().requires_grad_(True)

        # ğŸ”§ ä¼˜åŒ–ï¼šæ˜¾å¼æ¸…ç†ä¸­é—´å˜é‡ï¼Œå¸®åŠ©GCå›æ”¶ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
        del tensor_features, modality_masks_gpu, encoder_outputs, aligned_features, modality_presence, loss_dict
        
        # ğŸ”§ éªŒè¯3ï¼šå¼ºåˆ¶é‡Šæ”¾batchå¼•ç”¨ï¼ˆå¤šæ¨¡æ€ä»»åŠ¡ä¸­éå¸¸å…³é”®ï¼‰
        # å¼ºåˆ¶é‡Šæ”¾batchä¸­çš„æ‰€æœ‰å¼•ç”¨
        if isinstance(batch_features, dict):
            for k in list(batch_features.keys()):
                del batch_features[k]
        del batch_features
        if isinstance(modality_masks, dict):
            for k in list(modality_masks.keys()):
                del modality_masks[k]
        del modality_masks
        del batch  # é‡Šæ”¾æ•´ä¸ªbatchå…ƒç»„å¼•ç”¨
        
        # ğŸ”§ GC Optimization: Removed explicit gc.collect() calls.
        # Calling gc.collect() every N steps is a performance anti-pattern.
        # It pauses the entire training loop and clears CPU memory, not GPU memory.
        
        # æ³¨æ„ï¼šä¸åˆ é™¤total_lossï¼Œå› ä¸ºéœ€è¦è¿”å›

        return total_loss

    def validation_step(
        self,
        batch: Tuple[Dict[str, Union[np.ndarray, torch.Tensor]], Dict[str, Union[np.ndarray, torch.Tensor]], int],
        batch_idx: int
    ) -> torch.Tensor:
        """
        éªŒè¯æ­¥éª¤

        Args:
            batch: (batch_features, modality_masks, batch_size)å…ƒç»„
                batch_features: é¢„å¤„ç†åçš„ç‰¹å¾å­—å…¸ï¼Œé”®ä¸ºFeatureKeyï¼ˆå¯èƒ½æ˜¯numpyæ•°ç»„æˆ–tensorï¼‰
                modality_masks: æ¨¡æ€maskå­—å…¸ï¼Œé”®ä¸ºæ¨¡æ€åç§°ï¼ˆå¯èƒ½æ˜¯numpyæ•°ç»„æˆ–tensorï¼‰
                batch_size: batchå¤§å°ï¼ˆç”±collate_batchæä¾›ï¼Œé¿å…é‡å¤æ¨æ–­ï¼‰
            batch_idx: batchç´¢å¼•

        Returns:
            æŸå¤±å€¼
        """
        batch_features, modality_masks, batch_size = batch

        # ğŸ”§ ä¿®å¤ï¼švalidation_stepä¹Ÿéœ€è¦å°†numpyæ•°ç»„è½¬æ¢ä¸ºtensorï¼ˆä¸training_stepä¸€è‡´ï¼‰
        # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šåœ¨GPUä¸Šè½¬æ¢ä¸ºtensorï¼Œé¿å…åœ¨CPUä¸Šå ç”¨å†…å­˜
        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡è½¬æ¢ï¼Œå‡å°‘å¤šæ¬¡.to(device)è°ƒç”¨ï¼ˆä¸training_stepä¿æŒä¸€è‡´ï¼‰
        device = self.device
        
        # è½¬æ¢batch_featuresä¸ºGPU tensor
        tensor_features = {}
        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šå…ˆæ”¶é›†æ‰€æœ‰éœ€è¦è½¬æ¢çš„numpyæ•°ç»„ï¼Œç„¶åæ‰¹é‡è½¬æ¢
        numpy_items = []
        for key, feat in batch_features.items():
            if key == "_video_metadata" or key == "_modality_sources":
                tensor_features[key] = feat  # ä¿ç•™metadataå­—å…¸
                continue
            
            if feat is not None:
                if isinstance(feat, np.ndarray):
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨torch.as_tensorè¿›è¡Œé›¶æ‹·è´è½¬æ¢ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰éæ•´æ•°ç±»å‹éƒ½è½¬æ¢ä¸ºfloat32ï¼ˆmixed precisionè®­ç»ƒéœ€è¦ï¼‰
                    if feat.dtype == np.int64:
                        tensor_feat = torch.from_numpy(feat).long()
                    elif feat.dtype == np.int32:
                        tensor_feat = torch.from_numpy(feat).int()
                    else:
                        # å¤„ç†int8, uint8, int16, uint16, float16ç­‰ç±»å‹ï¼Œç»Ÿä¸€è½¬æ¢ä¸ºfloat32
                        if feat.dtype in [np.int8, np.uint8, np.int16, np.uint16, np.float16]:
                            tensor_feat = torch.from_numpy(feat.astype(np.float32)).float()
                        elif feat.dtype == np.float32:
                            tensor_feat = torch.as_tensor(feat).float()
                        else:
                            tensor_feat = torch.from_numpy(feat.astype(np.float32)).float()
                    numpy_items.append((key, tensor_feat))
                elif isinstance(feat, torch.Tensor):
                    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿tensorçš„dtypeæ­£ç¡®ï¼ˆmixed precisionè®­ç»ƒéœ€è¦float32/int64/int32/boolï¼‰
                    if feat.dtype not in [torch.float32, torch.int64, torch.int32, torch.bool]:
                        # å¦‚æœä¸æ˜¯æ ‡å‡†ç±»å‹ï¼Œè½¬æ¢ä¸ºfloat32
                        if feat.dtype in [torch.int8, torch.uint8, torch.int16, torch.uint16, torch.float16]:
                            feat = feat.float()
                        else:
                            feat = feat.float()
                    numpy_items.append((key, feat))
                else:
                    tensor_features[key] = feat
        
        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡ä¼ è¾“åˆ°GPUï¼Œä½¿ç”¨å¼‚æ­¥ä¼ è¾“ï¼ˆCUDAï¼‰æˆ–åŒæ­¥ä¼ è¾“ï¼ˆMPS/CPUï¼‰
        # ğŸ”§ ä¿®å¤ï¼šdeviceæ˜¯torch.deviceå¯¹è±¡ï¼Œä½¿ç”¨device.typeè€Œä¸æ˜¯startswith
        if device.type == "cuda":
            # CUDAè®¾å¤‡ï¼šä½¿ç”¨non_blockingå¼‚æ­¥ä¼ è¾“ï¼Œæé«˜æ•ˆç‡
            for key, tensor_feat in numpy_items:
                tensor_features[key] = tensor_feat.to(device, non_blocking=True)
        else:
            # MPSæˆ–CPUè®¾å¤‡ï¼šåŒæ­¥ä¼ è¾“
            for key, tensor_feat in numpy_items:
                tensor_features[key] = tensor_feat.to(device)
        
        # è½¬æ¢modality_masksä¸ºGPU tensor
        modality_masks_gpu = {}
        mask_items = []
        for k, v in modality_masks.items():
            if isinstance(v, np.ndarray):
                mask_items.append((k, torch.from_numpy(v).bool()))
            elif isinstance(v, torch.Tensor):
                # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿mask tensoræ˜¯boolç±»å‹
                if v.dtype != torch.bool:
                    v = v.bool()
                mask_items.append((k, v))
            else:
                modality_masks_gpu[k] = v
        
        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡ä¼ è¾“masksåˆ°GPUï¼Œä½¿ç”¨å¼‚æ­¥ä¼ è¾“ï¼ˆCUDAï¼‰
        # ğŸ”§ ä¿®å¤ï¼šdeviceæ˜¯torch.deviceå¯¹è±¡ï¼Œä½¿ç”¨device.typeè€Œä¸æ˜¯startswith
        if device.type == "cuda":
            for k, v in mask_items:
                modality_masks_gpu[k] = v.to(device, non_blocking=True)
        else:
            for k, v in mask_items:
                modality_masks_gpu[k] = v.to(device)

        # æ£€æŸ¥batchæ˜¯å¦ä¸ºç©º
        if not tensor_features:
            # ğŸ”§ æ”¹è¿›ï¼šæ”¹ä¸ºdebugçº§åˆ«ï¼Œå› ä¸ºè¿™æ˜¯é¢„æœŸçš„è¾¹ç•Œæƒ…å†µï¼ˆæ‰€æœ‰æ ·æœ¬mask=Falseæ—¶ï¼‰
            logger.debug(f"éªŒè¯æ­¥éª¤ {batch_idx}: tensor_features ä¸ºç©ºï¼ˆå¯èƒ½æ˜¯æ‰€æœ‰æ ·æœ¬mask=Falseï¼‰")
            return torch.tensor(0.0, device=self.device, dtype=torch.float32, requires_grad=False)

        # Batch ç¼–ç ï¼ˆåˆ©ç”¨ GPU å¹¶è¡Œèƒ½åŠ›ï¼‰
        try:
            encoder_outputs = self.encode_batch(tensor_features)
        except Exception as e:
            logger.error(f"éªŒè¯æ­¥éª¤ {batch_idx}: ç¼–ç å¤±è´¥ - {e}", exc_info=True)
            raise

        # å¯¹é½ç‰¹å¾ï¼ˆmodality_masks_gpuå·²åœ¨ä¸Šé¢è½¬æ¢ï¼‰
        try:
            aligned_features, modality_presence = self.forward(encoder_outputs, modality_masks_gpu)
        except Exception as e:
            logger.error(f"éªŒè¯æ­¥éª¤ {batch_idx}: å¯¹é½å¤±è´¥ - {e}", exc_info=True)
            raise

        # è®¡ç®—æŸå¤±
        try:
            # ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨forwardè¿”å›çš„modality_presenceï¼Œé¿å…é‡å¤æ¨æ–­
            total_loss, loss_dict = self.loss_fn(
                aligned_features,
                modality_presence=modality_presence,  # ä½¿ç”¨forwardæ¨æ–­çš„ç»“æœ
                logit_scales=self.aligner.get_logit_scales() if self.aligner.use_temperature_scaling else None,
                modality_masks=modality_masks_gpu  # Sample-level maskï¼Œç”¨äºç²¾ç¡®æ§åˆ¶æŸå¤±è®¡ç®—
            )
        except Exception as e:
            logger.error(f"éªŒè¯æ­¥éª¤ {batch_idx}: æŸå¤±è®¡ç®—å¤±è´¥ - {e}", exc_info=True)
            raise

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ¡ä»¶dtypeæ£€æŸ¥ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æˆ–å‡ºç°é—®é¢˜æ—¶æ£€æŸ¥ï¼‰
        if batch_idx == 0 or self._dtype_check_needed:
            if total_loss.dtype != torch.float32:
                logger.warning(f"éªŒè¯æ­¥éª¤ {batch_idx}: æŸå¤±dtypeä¸æ­£ç¡® ({total_loss.dtype})ï¼Œè½¬æ¢ä¸ºfloat32")
                total_loss = total_loss.float()
                self._dtype_check_needed = True
            else:
                self._dtype_check_needed = False
        
        # ç¡®ä¿æŸå¤±æœ‰æ•ˆ
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            logger.warning(f"éªŒè¯æ­¥éª¤ {batch_idx}: æŸå¤±ä¸º NaN æˆ– Infï¼Œä½¿ç”¨é›¶æŸå¤±")
            total_loss = torch.tensor(0.0, device=total_loss.device, dtype=torch.float32, requires_grad=False)

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šbatch_sizeå·²ä»collate_batchä¼ å…¥ï¼Œæ— éœ€é‡å¤æ¨æ–­
        # batch_sizeå·²åœ¨å‡½æ•°å¼€å§‹æ—¶ä»batchå…ƒç»„ä¸­è§£åŒ…

        # ä»…é€šè¿‡ Lightning çš„ MLflowLogger è®°å½•ï¼ˆç»Ÿä¸€ç”¨ MLflowï¼‰
        self.log("val_loss", total_loss, prog_bar=True, logger=True, on_step=False, on_epoch=True, batch_size=batch_size)
        for loss_name, loss_value in loss_dict.items():
            self.log(f"val_{loss_name}", loss_value, prog_bar=False, logger=True, on_step=False, on_epoch=True, batch_size=batch_size)

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šåªåœ¨æœ€åä¸€ä¸ªvalidation batchè®¡ç®—æŒ‡æ ‡ï¼Œé¿å…æ¯ä¸ªbatchéƒ½è®¡ç®—
        # ğŸ”§ ä¼˜åŒ–ï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦è®¡ç®—æŒ‡æ ‡ï¼Œä»¥åŠè®¡ç®—é¢‘ç‡
        should_compute_metrics = False
        if self.compute_val_metrics and self.trainer is not None:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªbatch
            is_last_batch = False
            if hasattr(self.trainer, 'is_last_batch'):
                is_last_batch = self.trainer.is_last_batch
            elif hasattr(self.trainer, 'num_val_batches') and self.trainer.num_val_batches is not None:
                if self.trainer.num_val_batches > 0:
                    is_last_batch = (batch_idx == self.trainer.num_val_batches - 1)
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœ¨è¿™ä¸ªepochè®¡ç®—æŒ‡æ ‡ï¼ˆæ ¹æ®val_metrics_every_n_epochsï¼‰
            if is_last_batch:
                current_epoch = self.trainer.current_epoch if hasattr(self.trainer, 'current_epoch') else 0
                if current_epoch % self.val_metrics_every_n_epochs == 0:
                    should_compute_metrics = True
        
        if should_compute_metrics:
            try:
                # ğŸ”§ ä¼˜åŒ–ï¼šè®°å½•æŒ‡æ ‡è®¡ç®—æ—¶é—´ï¼ˆç”¨äºæ€§èƒ½ç›‘æ§ï¼‰
                import time
                metrics_start_time = time.time()
                
                eval_metrics = self.metrics.compute_batch_metrics(
                    aligned_features,
                    modality_masks_gpu
                )
                
                metrics_time = time.time() - metrics_start_time
                # è®°å½•åˆ°SpeedMonitorï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if hasattr(self, 'trainer') and self.trainer is not None:
                    for callback in self.trainer.callbacks:
                        if hasattr(callback, 'val_metrics_times'):
                            callback.val_metrics_times.append(metrics_time)
                            # é™åˆ¶å†å²è®°å½•æ•°é‡
                            if len(callback.val_metrics_times) > callback.max_history:
                                callback.val_metrics_times = callback.val_metrics_times[-callback.max_history:]
                
                logger.debug(f"éªŒè¯æŒ‡æ ‡è®¡ç®—è€—æ—¶: {metrics_time*1000:.1f}ms")
                
                # è®°å½•è¯„ä¼°æŒ‡æ ‡ï¼ˆåªåœ¨epochç»“æŸæ—¶è®°å½•ï¼‰
                for metric_name, metric_value in eval_metrics.items():
                    self.log(
                        f"val_{metric_name}",
                        metric_value,
                        prog_bar=False,
                        logger=True,
                        on_step=False,
                        on_epoch=True,
                        batch_size=batch_size
                    )
            except Exception as e:
                logger.warning(f"éªŒè¯æ­¥éª¤ {batch_idx}: è¯„ä¼°æŒ‡æ ‡è®¡ç®—å¤±è´¥ - {e}")

        # ğŸ”§ ä¼˜åŒ–ï¼šæ˜¾å¼æ¸…ç†ä¸­é—´å˜é‡ï¼Œå¸®åŠ©GCå›æ”¶ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
        del tensor_features, modality_masks_gpu, encoder_outputs, aligned_features, modality_presence, loss_dict
        # æ³¨æ„ï¼šä¸åˆ é™¤total_lossï¼Œå› ä¸ºéœ€è¦è¿”å›

        return total_loss

    def configure_optimizers(self) -> Dict[str, Any]:
        """
        é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨

        Returns:
            åŒ…å«optimizerå’Œschedulerçš„å­—å…¸
        """
        # ä¼˜åŒ–å™¨ï¼šåªä¼˜åŒ–å¯¹é½æ¨¡å—çš„å‚æ•°ï¼ˆç¼–ç å™¨å·²å†»ç»“ï¼‰
        optimizer = torch.optim.AdamW(
            self.aligner.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šä½™å¼¦é€€ç«
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.config.epochs,
            eta_min=self.config.learning_rate * 0.1
        )

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "epoch",
                "frequency": 1
            }
        }

    def on_train_epoch_end(self) -> None:
        """Epoch ç»“æŸæ—¶è®°å½•èšåˆæŒ‡æ ‡ï¼Œç”± MLflowLogger å†™å…¥ MLflowã€‚"""
        if self.trainer is None:
            return
        cm = self.trainer.callback_metrics
        for key in ("train_loss", "train_loss_epoch"):
            if key in cm:
                self.log("train_loss_epoch", cm[key], prog_bar=False, logger=True, on_step=False, on_epoch=True)
                break
        if self.trainer.optimizers:
            lr = self.trainer.optimizers[0].param_groups[0].get("lr")
            if lr is not None:
                self.log("learning_rate_epoch", float(lr), prog_bar=False, logger=True, on_step=False, on_epoch=True)

    def on_validation_epoch_end(self) -> None:
        """Validation epoch ç»“æŸæ—¶è®°å½• val_lossï¼Œç”± MLflowLogger å†™å…¥ MLflowã€‚"""
        if self.trainer is None:
            return
        cm = self.trainer.callback_metrics
        for key in ("val_loss", "val_loss_epoch"):
            if key in cm:
                self.log("val_loss_epoch", cm[key], prog_bar=False, logger=True, on_step=False, on_epoch=True)
                break