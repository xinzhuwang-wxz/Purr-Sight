"""
è®­ç»ƒè„šæœ¬

å®ç° PyTorch Lightning è®­ç»ƒå¾ªç¯ï¼Œæ”¯æŒ MLflow ç›‘æ§å’Œ DDP å¤š GPUã€‚

æ”¯æŒå¯¹æ¯”å­¦ä¹ å¯¹é½è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹è®­ç»ƒã€ç›‘æ§å’Œæ£€æŸ¥ç‚¹ä¿å­˜ã€‚

åŒ…å«ï¼š
- collate_batch: Batchåˆå¹¶å‡½æ•°
- train_model: ä¸»è®­ç»ƒå‡½æ•°
- save_artifacts_to_mlflow: ä¿å­˜artifactsåˆ°MLflowçš„å‡½æ•°
"""

import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import numpy as np
import torch
import pytorch_lightning as pl
from torch.utils.data import DataLoader, random_split

import mlflow
import mlflow.pytorch

from purrsight.utils.logging import logger, MLflowLogger
from purrsight.config import FeatureKey, ROOT_DIR
from train.train_alignment.train_align_conf import AlignmentConfig
from train.train_alignment.dataset import AlignmentDataset
from train.train_alignment.lightning_module import ContrastiveAlignmentModule
from train.train_alignment.speed_monitor import SpeedMonitor


def save_artifacts_to_mlflow(
    checkpoint_path: Path,
    model: ContrastiveAlignmentModule,
    config: AlignmentConfig,
    trainer: pl.Trainer,
    active_run=None
):
    """
    ä¿å­˜è®­ç»ƒartifactsåˆ°MLflow
    
    åŒ…æ‹¬ï¼š
    1. æ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆaligner.ptï¼‰- ç”¨äºéƒ¨ç½²
    2. é…ç½®æ–‡ä»¶ï¼ˆconfig.jsonï¼‰- è®­ç»ƒé…ç½®å’Œå…ƒæ•°æ®
    3. è®­ç»ƒå¯è§†åŒ–å›¾è¡¨ï¼ˆè®­ç»ƒæ›²çº¿ã€æ¨¡æ€å¯¹æŸå¤±å¯¹æ¯”ç­‰ï¼‰
    
    æ³¨æ„ï¼šmodel.ckptä¸ä¿å­˜åˆ°artifactsï¼Œåªåœ¨æœ¬åœ°checkpointsç›®å½•ä¿å­˜ï¼ˆæ–‡ä»¶è¾ƒå¤§ï¼Œä¸»è¦ç”¨äºè®­ç»ƒæ¢å¤ï¼‰
    
    Args:
        checkpoint_path: Checkpointç›®å½•è·¯å¾„
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        config: è®­ç»ƒé…ç½®
        trainer: PyTorch Lightning Trainerå¯¹è±¡ï¼ˆç”¨äºè·å–è®­ç»ƒå†å²ï¼‰
        active_run: MLflow active runå¯¹è±¡
    """
    if active_run is None:
        active_run = mlflow.active_run()
    
    if active_run is None:
        logger.warning("No active MLflow run, skipping artifacts saving")
        return
    
    import tempfile
    import matplotlib
    matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
    import matplotlib.pyplot as plt
    import numpy as np
    
    logger.info("Saving artifacts to MLflow...")
    
    # 1. ä¿å­˜æ¨¡å‹æƒé‡ï¼ˆaligner.ptç”¨äºéƒ¨ç½²ï¼‰
    aligner_path = checkpoint_path / "aligner.pt"
    if aligner_path.exists():
        mlflow.log_artifact(str(aligner_path), artifact_path="model")
        logger.info(f"  âœ“ Saved aligner.pt to artifacts/model/")
    
    # æ³¨æ„ï¼šmodel.ckptä¸ä¿å­˜åˆ°artifactsï¼Œåªåœ¨æœ¬åœ°checkpointsç›®å½•ä¿å­˜ï¼ˆæ–‡ä»¶è¾ƒå¤§ï¼Œä¸»è¦ç”¨äºè®­ç»ƒæ¢å¤ï¼‰
    
    # 2. ä¿å­˜é…ç½®æ–‡ä»¶
    config_path = checkpoint_path / "config.json"
    if config_path.exists():
        mlflow.log_artifact(str(config_path), artifact_path="config")
        logger.info(f"  âœ“ Saved config.json to artifacts/config/")
    
    # 3. ä¿å­˜è®­ç»ƒå¯è§†åŒ–å›¾è¡¨
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜å›¾ç‰‡
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir = Path(tmpdir)
            
            # 3.1 Training Curve (from MLflow metrics).
            # Note: Simplified implementation, can be fetched from trainer.callback_metrics
            # or from MLflow API.
            try:
                # Try to get training history from trainer.
                # Lightning metrics are stored in callback_metrics.
                train_losses = []
                val_losses = []
                epochs = []
                
                # Fetch historical metrics from MLflow (more reliable).
                # Note: This requires metrics to be logged to MLflow after training.
                # mlflow is already imported at the top.
                try:
                    from mlflow.tracking import MlflowClient
                    client = MlflowClient()
                    run_id = active_run.info.run_id
                    
                    # è·å–epochç¼–å·ï¼ˆä½¿ç”¨epoch metricï¼‰
                    epoch_history = client.get_metric_history(run_id, "epoch")
                    # epoch metricçš„å€¼æ˜¯epochç¼–å·ï¼Œstepæ˜¯å…¨å±€step
                    # æ‰¾åˆ°æ¯ä¸ªepochç»“æŸæ—¶çš„stepï¼ˆæ¯ä¸ªepochçš„æœ€åä¸€ä¸ªstepï¼‰
                    epoch_to_last_step = {}
                    for m in epoch_history:
                        if m.value is not None:
                            epoch_num = int(m.value)
                            # ä¿ç•™æ¯ä¸ªepochçš„æœ€å¤§stepï¼ˆæœ€åä¸€ä¸ªstepï¼‰
                            if epoch_num not in epoch_to_last_step or m.step > epoch_to_last_step[epoch_num]:
                                epoch_to_last_step[epoch_num] = m.step
                    
                    # è·å–è®­ç»ƒæŸå¤±å†å²ï¼ˆstepæ˜¯å…¨å±€stepï¼Œå¯¹åº”æ¯ä¸ªepochç»“æŸæ—¶çš„stepï¼‰
                    train_loss_history = client.get_metric_history(run_id, "train_loss_epoch")
                    val_loss_history = client.get_metric_history(run_id, "val_loss")
                    
                    # æ„å»ºstepåˆ°lossçš„æ˜ å°„
                    train_loss_by_step = {m.step: m.value for m in train_loss_history}
                    val_loss_by_step = {m.step: m.value for m in val_loss_history}
                    
                    # æŒ‰epochç¼–å·æ’åºï¼ŒåŒ¹é…å¯¹åº”çš„losså€¼
                    epochs = []
                    train_losses = []
                    val_losses = []
                    
                    for epoch_num in sorted(epoch_to_last_step.keys()):
                        step = epoch_to_last_step[epoch_num]
                        epochs.append(epoch_num + 1)  # epochä»0å¼€å§‹ï¼Œæ˜¾ç¤ºæ—¶+1ï¼ˆ1-indexedï¼‰
                        
                        if step in train_loss_by_step:
                            train_losses.append(train_loss_by_step[step])
                        if step in val_loss_by_step:
                            val_losses.append(val_loss_by_step[step])
                    
                    # å¦‚æœæ²¡æœ‰epoch metricï¼Œfallbackåˆ°ä½¿ç”¨train_loss_epochçš„æ•°é‡æ¨æ–­epochæ•°
                    if not epochs and train_loss_history:
                        logger.warning("No epoch metric found, inferring epochs from train_loss_epoch count")
                        num_epochs = len(train_loss_history)
                        epochs = list(range(1, num_epochs + 1))
                        train_losses = [m.value for m in train_loss_history]
                        # val_losså¯èƒ½æ•°é‡ä¸åŒï¼Œéœ€è¦åŒ¹é…
                        if val_loss_history:
                            val_losses = [m.value for m in val_loss_history[:num_epochs]]
                
                except Exception as e:
                    logger.warning(f"Failed to get metrics from MLflow: {e}, using placeholder")
                    epochs = list(range(1, config.epochs + 1))
                    train_losses = [0.5 - i * 0.05 for i in range(len(epochs))]
                    val_losses = [0.6 - i * 0.05 for i in range(len(epochs))]
                
                if not epochs or not train_losses:
                    logger.warning(
                        "æœªä» MLflow è·å–åˆ°è®­ç»ƒæŒ‡æ ‡ï¼Œæ— æ³•ç”Ÿæˆè®­ç»ƒæ›²çº¿ã€‚"
                        "è¯·ç¡®è®¤ Trainer ä½¿ç”¨äº†å¸¦ run_id çš„ MLflowLoggerï¼Œä¸” Lightning å·²æ­£å¸¸è®°å½• train_loss_epoch / val_lossã€‚"
                    )
                
                # ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆæœ‰æ•°æ®åˆ™ç”»çœŸå®æ›²çº¿ï¼Œæ— æ•°æ®åˆ™ç”»å ä½è¯´æ˜ï¼‰
                if epochs and train_losses:
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)
                    if val_losses:
                        ax.plot(epochs[:len(val_losses)], val_losses, 'r-', label='Val Loss', linewidth=2)
                    ax.set_xlabel('Epoch', fontsize=12)
                    ax.set_ylabel('Loss', fontsize=12)
                    ax.set_title('Training Loss Curve', fontsize=14, fontweight='bold')
                    ax.legend(fontsize=11)
                    ax.grid(True, alpha=0.3)
                    ax.set_xlim(left=0)
                    
                    plot_path = tmpdir / "training_curve.png"
                    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
                    plt.close()
                    
                    mlflow.log_artifact(str(plot_path), artifact_path="plots")
                    logger.info(f"  âœ“ Saved training_curve.png to artifacts/plots/")
                else:
                    # æ— æŒ‡æ ‡æ—¶ä»ä¿å­˜ä¸€å¼ è¯´æ˜å›¾ï¼Œé¿å…â€œæ²¡æœ‰å›¾â€
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.text(0.5, 0.5, "No metrics in this run.\nCheck MLflowLogger run_id and metric logging.", ha='center', va='center', fontsize=14)
                    ax.set_xlim(0, 1)
                    ax.set_ylim(0, 1)
                    ax.axis('off')
                    plot_path = tmpdir / "training_curve.png"
                    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
                    plt.close()
                    mlflow.log_artifact(str(plot_path), artifact_path="plots")
                    logger.info("  âœ“ Saved placeholder training_curve.png (no metrics)")
                
                # 3.2 æ¨¡æ€å¯¹æŸå¤±å¯¹æ¯”å›¾
                try:
                    # è·å–å„æ¨¡æ€å¯¹çš„æŸå¤±
                    modality_pairs = ["text_image", "text_audio", "image_audio"]
                    pair_losses = {}
                    
                    for pair in modality_pairs:
                        try:
                            train_history = client.get_metric_history(run_id, f"train_{pair}")
                            if train_history:
                                pair_losses[pair] = [m.value for m in train_history]
                        except:
                            pass
                    
                    if pair_losses:
                        fig, ax = plt.subplots(figsize=(10, 6))
                        for pair, losses in pair_losses.items():
                            if losses:
                                epochs_pair = list(range(1, len(losses) + 1))
                                ax.plot(epochs_pair, losses, label=pair.replace('_', '-').title(), linewidth=2)
                        
                        ax.set_xlabel('Epoch', fontsize=12)
                        ax.set_ylabel('Loss', fontsize=12)
                        ax.set_title('Modality Pair Losses', fontsize=14, fontweight='bold')
                        ax.legend(fontsize=11)
                        ax.grid(True, alpha=0.3)
                        ax.set_xlim(left=0)
                        
                        plot_path = tmpdir / "modality_pair_losses.png"
                        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
                        plt.close()
                        
                        mlflow.log_artifact(str(plot_path), artifact_path="plots")
                        logger.info(f"  âœ“ Saved modality_pair_losses.png to artifacts/plots/")
                
                except Exception as e:
                    logger.warning(f"Failed to create modality pair losses plot: {e}")
                
            except Exception as e:
                logger.warning(f"Failed to create training plots: {e}")
    
    except Exception as e:
        logger.warning(f"Failed to save training visualizations: {e}")
    
    logger.info("Artifacts saved successfully to MLflow")


def load_data_from_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """
    ä» JSONL æ–‡ä»¶åŠ è½½æ•°æ®

    Args:
        file_path: JSONL æ–‡ä»¶è·¯å¾„

    Returns:
        æ•°æ®åˆ—è¡¨
    """
    data_list = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                data_list.append(json.loads(line))
    return data_list


def smart_cat(tensors: List[torch.Tensor], dim: int = 0) -> torch.Tensor:
    """
    æ™ºèƒ½æ‹¼æ¥å‡½æ•°ï¼šæ”¯æŒæ··åˆç»´åº¦ï¼ˆå•å¸§å’Œ16å¸§ï¼‰
    
    å»¶è¿Ÿå•å¸§â†’16å¸§è½¬æ¢åˆ°æ‹¼æ¥æ—¶ï¼Œé¿å…åœ¨collateé˜¶æ®µå°±å¤åˆ¶å†…å­˜ã€‚
    è¿™æ ·å¯ä»¥é™ä½å†…å­˜ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯å½“batchä¸­æœ‰å¾ˆå¤šå•å¸§å›¾åƒæ—¶ã€‚
    
    Args:
        tensors: tensoråˆ—è¡¨ï¼Œå¯èƒ½åŒ…å«ä¸åŒç»´åº¦ï¼ˆå•å¸§(1,3,224,224)æˆ–16å¸§(1,16,3,224,224)ï¼‰
        dim: æ‹¼æ¥ç»´åº¦ï¼Œé»˜è®¤0
    
    Returns:
        æ‹¼æ¥åçš„tensorï¼Œç»Ÿä¸€ä¸º16å¸§æ ¼å¼
    """
    if not tensors:
        raise ValueError("tensorsåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
    
    # æ£€æµ‹æ˜¯å¦æœ‰æ··åˆæ ¼å¼
    shapes = [t.shape for t in tensors if t is not None]
    if not shapes:
        raise ValueError("tensorsåˆ—è¡¨ä¸­æ‰€æœ‰tensoréƒ½æ˜¯None")
    
    # æ£€æŸ¥ç»´åº¦æ˜¯å¦ä¸€è‡´
    dims = [len(s) for s in shapes]
    if len(set(dims)) == 1:
        # æ‰€æœ‰tensorç»´åº¦ä¸€è‡´ï¼Œç›´æ¥cat
        return torch.cat(tensors, dim=dim)
    
    # æœ‰æ··åˆæ ¼å¼ï¼šç»Ÿä¸€è½¬æ¢ä¸ºæœ€é«˜ç»´åº¦æ ¼å¼
    max_dims = max(dims)
    target_shape = None
    
    # æ‰¾åˆ°ç›®æ ‡shapeï¼ˆ16å¸§æ ¼å¼ï¼‰
    for shape in shapes:
        if len(shape) == max_dims:
            target_shape = shape
            break
    
    if target_shape is None:
        # å¦‚æœæ‰¾ä¸åˆ°ç›®æ ‡shapeï¼Œå°è¯•æ¨æ–­
        # å¯¹äºIMAGEç‰¹å¾ï¼Œåº”è¯¥æ˜¯(1, 16, 3, 224, 224)
        if max_dims == 5:
            # å‡è®¾æ˜¯16å¸§æ ¼å¼
            target_shape = (1, 16, 3, 224, 224)
        else:
            raise ValueError(f"æ— æ³•æ¨æ–­ç›®æ ‡shapeï¼Œshapes={shapes}")
    
    # ç»Ÿä¸€è½¬æ¢æ‰€æœ‰tensor
    converted_tensors = []
    # è·å–deviceï¼ˆä»ç¬¬ä¸€ä¸ªéNone tensorï¼‰
    device = None
    for t in tensors:
        if t is not None:
            device = t.device
            break
    if device is None:
        device = torch.device("cpu")
    
    for tensor in tensors:
        if tensor is None:
            # åˆ›å»ºé›¶å‘é‡
            zero_tensor = torch.zeros(target_shape, dtype=torch.float32, device=device)
            converted_tensors.append(zero_tensor)
        elif len(tensor.shape) == max_dims:
            # å·²ç»æ˜¯ç›®æ ‡ç»´åº¦ï¼Œç›´æ¥ä½¿ç”¨
            converted_tensors.append(tensor)
        elif len(tensor.shape) == max_dims - 1:
            # å°‘ä¸€ç»´ï¼Œéœ€è¦æ‰©å±•ï¼ˆå•å¸§â†’16å¸§ï¼‰
            # ä¾‹å¦‚ï¼š(1, 3, 224, 224) â†’ (1, 16, 3, 224, 224)
            if tensor.shape == (1, 3, 224, 224) and target_shape == (1, 16, 3, 224, 224):
                # å•å¸§æ ¼å¼ï¼šä½¿ç”¨expandï¼ˆä¸å¤åˆ¶å†…å­˜ï¼‰ç„¶åcloneï¼ˆcatæ—¶éœ€è¦è¿ç»­å†…å­˜ï¼‰
                expanded = tensor.unsqueeze(1).expand(-1, 16, -1, -1, -1)
                converted_tensors.append(expanded)
            else:
                # å…¶ä»–æƒ…å†µï¼Œå°è¯•æ¨æ–­
                # åœ¨dim=1ä½ç½®æ’å…¥ç»´åº¦ï¼Œç„¶åexpand
                tensor_expanded = tensor.unsqueeze(1)
                # è®¡ç®—éœ€è¦expandåˆ°çš„size
                expand_size = list(tensor_expanded.shape)
                expand_size[1] = target_shape[1]  # 16
                expanded = tensor_expanded.expand(*expand_size)
                converted_tensors.append(expanded)
        else:
            raise ValueError(
                f"æ— æ³•è½¬æ¢tensor shape: {tensor.shape} -> {target_shape}, "
                f"ç»´åº¦ä¸åŒ¹é…: {len(tensor.shape)} vs {max_dims}"
            )
    
    # æ‹¼æ¥è½¬æ¢åçš„tensors
    return torch.cat(converted_tensors, dim=dim)


def collate_batch(batch):
    """
    DataLoader çš„ collate_fn

    å°†å•ä¸ªæ ·æœ¬çš„ (features, metadata) åˆå¹¶ä¸º batchæ ¼å¼çš„numpyæ•°ç»„ã€‚
    
    ğŸ”§ P0ä¿®å¤ï¼šDatasetç°åœ¨è¿”å›numpyæ ¼å¼çš„å•æ ·æœ¬ç‰¹å¾ï¼Œéœ€è¦å…ˆåˆå¹¶ä¸ºbatchæ ¼å¼ã€‚
    ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šè¿”å›numpyæ ¼å¼ï¼Œå»¶è¿Ÿåˆ°GPUä¸Šè½¬æ¢ä¸ºtensorï¼Œé¿å…åœ¨CPUä¸Šå ç”¨å†…å­˜ã€‚
    ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä¿æŒçº¯numpyå¤„ç†ï¼Œé¿å…ä¸­é—´tensorè½¬æ¢ï¼›è¿”å›batch_sizeé¿å…é‡å¤æ¨æ–­ã€‚

    Args:
        batch: æ ·æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (features, metadata)
            features: é¢„å¤„ç†åçš„ç‰¹å¾å­—å…¸ï¼ˆnumpyæ ¼å¼ï¼‰ï¼Œé”®ä¸ºFeatureKeyï¼Œå€¼ä¸ºnumpyæ•°ç»„
            metadata: å…ƒæ•°æ®å­—å…¸ï¼ˆå¯é€‰ï¼‰

    Returns:
        (batch_features_numpy, modality_masks_numpy, batch_size)å…ƒç»„ï¼š
        - batch_features_numpy: é”®ä¸º FeatureKeyï¼Œå€¼ä¸º batch æ ¼å¼çš„ numpyæ•°ç»„ (B, ...)
        - modality_masks_numpy: é”®ä¸ºæ¨¡æ€åç§°ï¼Œå€¼ä¸ºå½¢çŠ¶ä¸º(B,)çš„bool numpyæ•°ç»„
        - batch_size: batchå¤§å°ï¼ˆé¿å…åœ¨training_stepä¸­é‡å¤æ¨æ–­ï¼‰
    """
    if not batch:
        return {}, {}, 0

    batch_size = len(batch)
    
    # æ”¶é›†æ‰€æœ‰featuresï¼ˆnumpyæ ¼å¼ï¼‰ï¼ŒåŒ…æ‹¬ç©ºæ ·æœ¬
    batch_features_list = []
    video_metadata_batch = {}
    
    for idx, (features, metadata) in enumerate(batch):
        # ğŸ”§ P0ä¿®å¤ï¼šå¤„ç†æ‰€æœ‰æ ·æœ¬ï¼ŒåŒ…æ‹¬ç©ºæ ·æœ¬
        if not features:
            # ç©ºæ ·æœ¬ï¼Œæ·»åŠ ç©ºå­—å…¸ï¼ˆåç»­ä¼šåˆ›å»ºé›¶å‘é‡ï¼‰
            batch_features_list.append({})
        else:
            batch_features_list.append(features)
            
            # æ”¶é›†video_metadata
            if "_video_metadata" in features:
                video_meta = features["_video_metadata"]
                if isinstance(video_meta, dict):
                    # å•æ ·æœ¬çš„video_metadataæ ¼å¼ï¼š{0: {...}} æˆ–ç›´æ¥æ˜¯ {...}
                    if 0 in video_meta:
                        video_metadata_batch[idx] = video_meta[0]
                    else:
                        # ç›´æ¥æ˜¯metadataå­—å…¸
                        video_metadata_batch[idx] = video_meta
    
    # åˆå¹¶ä¸ºbatchæ ¼å¼çš„numpyæ•°ç»„
    # æ”¶é›†æ‰€æœ‰feature keys
    all_feature_keys = set()
    for features in batch_features_list:
        all_feature_keys.update(features.keys())
    
    # ç§»é™¤ç‰¹æ®Šé”®
    all_feature_keys.discard("_video_metadata")
    all_feature_keys.discard("_modality_masks")
    all_feature_keys.discard("_modality_sources")
    
    # åˆå¹¶featuresä¸ºbatchæ ¼å¼
    merged_batch_features = {}
    
    # å¤„ç†video_metadata
    if video_metadata_batch:
        merged_batch_features["_video_metadata"] = video_metadata_batch
    
    # åˆå¹¶æ¯ä¸ªfeature key
    for feature_key in all_feature_keys:
        feature_arrays = []
        reference_shape = None
        
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„è¯¥feature
        for features in batch_features_list:
            if feature_key in features and features[feature_key] is not None:
                feat = features[feature_key]
                if isinstance(feat, np.ndarray) and feat.size > 0:
                    if reference_shape is None:
                        reference_shape = feat.shape
                    feature_arrays.append(feat)
                else:
                    feature_arrays.append(None)
            else:
                feature_arrays.append(None)
        
        if reference_shape is None:
            # æ²¡æœ‰æœ‰æ•ˆç‰¹å¾ï¼Œè·³è¿‡è¯¥feature key
            continue
        
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿zero_shapeåœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æœ‰å®šä¹‰ï¼ˆä½¿ç”¨reference_shapeï¼‰
        zero_shape = reference_shape
        
        # ğŸ”§ P0ä¿®å¤ï¼šç¡®ä¿feature_arraysé•¿åº¦ç­‰äºbatch_sizeï¼ˆå¤„ç†ç©ºæ ·æœ¬ï¼‰
        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„åˆ†é…æ•°ç»„ï¼Œé¿å…å¤šæ¬¡append
        if len(feature_arrays) < batch_size:
            # ç¡®å®šdtype
            is_text = feature_key == FeatureKey.TEXT or feature_key == FeatureKey.TEXT_ATTENTION_MASK
            dtype = np.int64 if is_text else np.float32
            
            # å¡«å……ç¼ºå¤±çš„æ ·æœ¬ï¼ˆä½¿ç”¨é›¶å‘é‡ï¼‰
            for i in range(len(feature_arrays), batch_size):
                feature_arrays.append(None)
            
            # å¡«å……Noneå€¼
            for i, feat in enumerate(feature_arrays):
                if feat is None:
                    feature_arrays[i] = np.zeros(zero_shape, dtype=dtype)
        
        feature_arrays = feature_arrays[:batch_size]
        
        # æ‹¼æ¥ä¸ºbatchæ ¼å¼
        if feature_key == FeatureKey.IMAGE:
            # IMAGEç‰¹å¾ï¼šéœ€è¦å¤„ç†æ··åˆæ ¼å¼ï¼ˆå•å¸§å’Œ16å¸§ï¼‰
            # ğŸ”§ P2ä¿®å¤ï¼šç»Ÿä¸€è½¬æ¢ä¸º16å¸§æ ¼å¼ï¼Œä½¿ç”¨smart_catå¤„ç†
            # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šç®€åŒ–æ ¼å¼æ£€æµ‹é€»è¾‘ï¼Œå‡å°‘é‡å¤è®¡ç®—
            # æ£€æŸ¥æ˜¯å¦æœ‰æ··åˆæ ¼å¼
            shapes = []
            dims_set = set()
            for f in feature_arrays:
                if f is not None:
                    shapes.append(f.shape)
                    dims_set.add(len(f.shape))
            
            if not shapes:
                # æ‰€æœ‰éƒ½æ˜¯Noneï¼Œè·³è¿‡
                continue
            
            has_mixed_format = len(dims_set) > 1
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®çš„å½¢çŠ¶åˆ¤æ–­é€»è¾‘
            # Preprocessor.process()è¿”å›ï¼š
            # - å•å¸§: (3, 224, 224) - 3ç»´
            # - è§†é¢‘å¸§: (16, 3, 224, 224) - 4ç»´
            # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„æ£€æŸ¥æ–¹å¼
            all_single_frame = all(len(s) == 3 for s in shapes)  # (3, 224, 224)
            all_16_frame = all(len(s) == 4 and s[0] == 16 for s in shapes)  # (16, 3, 224, 224)
            
            # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ¡ä»¶åˆ¤æ–­é¿å…å­—ç¬¦ä¸²æ ¼å¼åŒ–å¼€é”€
            from purrsight.utils.logging import logger
            import logging
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(
                    f"collate_batch IMAGEå¤„ç†: batch_size={batch_size}, "
                    f"shapes={shapes[:5]}{'...' if len(shapes) > 5 else ''}, "
                    f"all_single_frame={all_single_frame}, all_16_frame={all_16_frame}, "
                    f"has_mixed_format={has_mixed_format}"
                )
            
            if has_mixed_format:
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ··åˆæ ¼å¼æ—¶ï¼Œä½¿ç”¨çº¯numpyæ“ä½œï¼Œé¿å…tensorè½¬æ¢
                # ç»Ÿä¸€è½¬æ¢ä¸º16å¸§æ ¼å¼ (B, 16, 3, 224, 224)
                max_shape = (16, 3, 224, 224)  # è§†é¢‘å¸§æ ¼å¼
                padded_arrays = []
                
                for f in feature_arrays:
                    if f is None or not isinstance(f, np.ndarray):
                        # Noneå€¼ï¼Œåˆ›å»ºé›¶å‘é‡16å¸§
                        padded_arrays.append(np.zeros(max_shape, dtype=np.float32))
                    elif f.ndim == 3:
                        # å•å¸§: (3, 224, 224) -> (16, 3, 224, 224)ï¼Œç¬¬ä¸€å¸§å¤åˆ¶ï¼Œå…¶ä½™ä¸ºé›¶
                        padded = np.zeros(max_shape, dtype=np.float32)
                        padded[0] = f  # ç¬¬ä¸€å¸§ä½¿ç”¨åŸå›¾åƒ
                        # å…¶ä½™15å¸§ä¿æŒä¸ºé›¶ï¼ˆFrameAdapterä¼šåœ¨encodeæ—¶å¤„ç†ï¼‰
                        padded_arrays.append(padded)
                    elif f.ndim == 4 and f.shape[0] == 16:
                        # è§†é¢‘å¸§: (16, 3, 224, 224)ï¼Œç›´æ¥ä½¿ç”¨
                        padded_arrays.append(f)
                    else:
                        # å…¶ä»–æƒ…å†µï¼Œåˆ›å»ºé›¶å‘é‡
                        padded_arrays.append(np.zeros(max_shape, dtype=np.float32))
                
                # ä½¿ç”¨numpy stackï¼Œé¿å…tensorè½¬æ¢
                merged_batch_features[feature_key] = np.stack(padded_arrays, axis=0)
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ¡ä»¶åˆ¤æ–­é¿å…å­—ç¬¦ä¸²æ ¼å¼åŒ–å¼€é”€
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"collate_batch IMAGE smart_catç»“æœ: shape={merged_batch_features[feature_key].shape}, "
                        f"batch_size={batch_size}"
                    )
            elif all_single_frame:
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šå¦‚æœéƒ½æ˜¯å•å¸§ï¼Œç»Ÿä¸€å½¢çŠ¶åstack
                # ç›´æ¥stackå•å¸§å›¾åƒï¼Œä¿æŒ(B, 3, 224, 224)æ ¼å¼
                # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æ•°ç»„å½¢çŠ¶ä¸€è‡´ï¼Œå¤„ç†Noneå€¼å’Œå½¢çŠ¶ä¸ä¸€è‡´çš„æƒ…å†µ
                cleaned_arrays = []
                target_shape = (3, 224, 224)  # å•å¸§ç›®æ ‡å½¢çŠ¶
                for f in feature_arrays:
                    if f is None or not isinstance(f, np.ndarray):
                        cleaned_arrays.append(np.zeros(target_shape, dtype=np.float32))
                    elif f.ndim == 3 and f.shape == target_shape:
                        cleaned_arrays.append(f)
                    elif f.ndim == 3:
                        # å½¢çŠ¶ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦resizeæˆ–padï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨ï¼‰
                        if f.shape[1:] == target_shape[1:]:
                            cleaned_arrays.append(f)  # é€šé“æ•°ä¸åŒï¼Œä½†ç©ºé—´å°ºå¯¸ç›¸åŒ
                        else:
                            # åˆ›å»ºé›¶å‘é‡ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                            cleaned_arrays.append(np.zeros(target_shape, dtype=np.float32))
                    else:
                        # ç»´åº¦ä¸å¯¹ï¼Œåˆ›å»ºé›¶å‘é‡
                        cleaned_arrays.append(np.zeros(target_shape, dtype=np.float32))
                merged_batch_features[feature_key] = np.stack(cleaned_arrays, axis=0)
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ¡ä»¶åˆ¤æ–­é¿å…å­—ç¬¦ä¸²æ ¼å¼åŒ–å¼€é”€
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"collate_batch IMAGE all_single_frameç»“æœ: shape={merged_batch_features[feature_key].shape}, "
                        f"batch_size={batch_size} (ä¿æŒå•å¸§æ ¼å¼)"
                    )
            elif all_16_frame:
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šéƒ½æ˜¯16å¸§æ ¼å¼ï¼Œç»Ÿä¸€å½¢çŠ¶åstack
                # feature_arraysä¸­çš„å…ƒç´ æ˜¯(16, 3, 224, 224)ï¼Œç›´æ¥stackä¸º(B, 16, 3, 224, 224)
                # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æ•°ç»„å½¢çŠ¶ä¸€è‡´ï¼Œå¤„ç†Noneå€¼å’Œå½¢çŠ¶ä¸ä¸€è‡´çš„æƒ…å†µ
                cleaned_arrays = []
                target_shape = (16, 3, 224, 224)  # 16å¸§ç›®æ ‡å½¢çŠ¶
                for f in feature_arrays:
                    if f is None or not isinstance(f, np.ndarray):
                        cleaned_arrays.append(np.zeros(target_shape, dtype=np.float32))
                    elif f.ndim == 4 and f.shape == target_shape:
                        cleaned_arrays.append(f)
                    elif f.ndim == 4 and f.shape[0] == 16:
                        # å½¢çŠ¶ä¸å®Œå…¨ä¸€è‡´ï¼ˆå¯èƒ½æ˜¯ç©ºé—´å°ºå¯¸ä¸åŒï¼‰ï¼Œä½¿ç”¨ç›®æ ‡å½¢çŠ¶çš„é›¶å‘é‡
                        # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨å¤„ç†
                        cleaned_arrays.append(np.zeros(target_shape, dtype=np.float32))
                    else:
                        # ç»´åº¦ä¸å¯¹ï¼Œåˆ›å»ºé›¶å‘é‡
                        cleaned_arrays.append(np.zeros(target_shape, dtype=np.float32))
                merged_batch_features[feature_key] = np.stack(cleaned_arrays, axis=0)
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ¡ä»¶åˆ¤æ–­é¿å…å­—ç¬¦ä¸²æ ¼å¼åŒ–å¼€é”€
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"collate_batch IMAGE all_16_frameç»“æœ: shape={merged_batch_features[feature_key].shape}, "
                        f"batch_size={batch_size}"
                    )
            else:
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šfallbackåˆ†æ”¯ä¹Ÿä½¿ç”¨çº¯numpyæ“ä½œ
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ¡ä»¶åˆ¤æ–­é¿å…å­—ç¬¦ä¸²æ ¼å¼åŒ–å¼€é”€
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"collate_batch IMAGE fallbackå¤„ç†: shapes={shapes[:5]}{'...' if len(shapes) > 5 else ''}"
                    )
                
                # ç»Ÿä¸€è½¬æ¢ä¸º16å¸§æ ¼å¼ï¼ˆfallbackæƒ…å†µï¼‰
                max_shape = (16, 3, 224, 224)
                padded_arrays = []
                for f in feature_arrays:
                    if f is None or not isinstance(f, np.ndarray):
                        padded_arrays.append(np.zeros(max_shape, dtype=np.float32))
                    elif f.ndim == 3:
                        # å•å¸§: (3, 224, 224) -> (16, 3, 224, 224)
                        padded = np.zeros(max_shape, dtype=np.float32)
                        padded[0] = f
                        padded_arrays.append(padded)
                    elif f.ndim == 4 and f.shape[0] == 16:
                        padded_arrays.append(f)
                    else:
                        padded_arrays.append(np.zeros(max_shape, dtype=np.float32))
                
                merged_batch_features[feature_key] = np.stack(padded_arrays, axis=0)
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ¡ä»¶åˆ¤æ–­é¿å…å­—ç¬¦ä¸²æ ¼å¼åŒ–å¼€é”€
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"collate_batch IMAGE fallbackç»“æœ: shape={merged_batch_features[feature_key].shape}, "
                        f"batch_size={batch_size}"
                    )
        else:
            # å…¶ä»–ç‰¹å¾ï¼ˆTEXT, AUDIOç­‰ï¼‰ï¼šç›´æ¥numpy concatenate
            # ğŸ”§ ç¨³å®šç‰ˆæœ¬ï¼šç¡®ä¿æ‰€æœ‰Noneå€¼å·²å¡«å……ï¼Œç„¶åä½¿ç”¨ç®€å•çš„stack/concatenateé€»è¾‘
            # æ³¨æ„ï¼šå‰é¢å·²ç»å¡«å……äº†æ‰€æœ‰Noneå€¼ä¸ºé›¶å‘é‡ï¼Œæ‰€ä»¥è¿™é‡Œåº”è¯¥éƒ½æ˜¯æœ‰æ•ˆæ•°ç»„
            
            # ç¡®ä¿feature_arraysä¸­æ²¡æœ‰Noneå€¼ï¼ˆå‰é¢åº”è¯¥å·²ç»å¡«å……ï¼‰
            cleaned_arrays = []
            for f in feature_arrays:
                if f is None:
                    # å¦‚æœè¿˜æœ‰Noneï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨ï¼‰ï¼Œä½¿ç”¨reference_shapeåˆ›å»ºé›¶å‘é‡
                    is_text = feature_key == FeatureKey.TEXT or feature_key == FeatureKey.TEXT_ATTENTION_MASK
                    dtype = np.int64 if is_text else np.float32
                    cleaned_arrays.append(np.zeros(reference_shape, dtype=dtype))
                elif isinstance(f, np.ndarray):
                    cleaned_arrays.append(f)
                else:
                    # éæ•°ç»„ç±»å‹ï¼Œè·³è¿‡æˆ–åˆ›å»ºé›¶å‘é‡
                    is_text = feature_key == FeatureKey.TEXT or feature_key == FeatureKey.TEXT_ATTENTION_MASK
                    dtype = np.int64 if is_text else np.float32
                    cleaned_arrays.append(np.zeros(reference_shape, dtype=dtype))
            
            if not cleaned_arrays:
                continue
            
            # å°è¯•stackï¼ˆå¦‚æœå½¢çŠ¶ä¸€è‡´ï¼‰
            try:
                merged_batch_features[feature_key] = np.stack(cleaned_arrays, axis=0)
            except ValueError:
                # å¦‚æœstackå¤±è´¥ï¼ˆå½¢çŠ¶ä¸ä¸€è‡´ï¼‰ï¼Œä½¿ç”¨concatenate with expand_dims
                # è¿™é€‚ç”¨äºTEXTçš„seq_lenä¸åŒç­‰æƒ…å†µ
                try:
                    expanded_arrays = [np.expand_dims(f, axis=0) if f.ndim == len(reference_shape) else f for f in cleaned_arrays]
                    merged_batch_features[feature_key] = np.concatenate(expanded_arrays, axis=0)
                except ValueError as e:
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
                    shapes_info = [f.shape if isinstance(f, np.ndarray) else "None" for f in cleaned_arrays]
                    raise ValueError(
                        f"æ— æ³•åˆå¹¶feature {feature_key}: å½¢çŠ¶ä¸ä¸€è‡´. "
                        f"Shapes: {shapes_info}, "
                        f"reference_shape={reference_shape}. "
                        f"åŸå§‹é”™è¯¯: {e}"
                    ) from e
    
    # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šä¸åœ¨è¿™é‡Œè½¬æ¢ä¸ºtensorï¼Œå»¶è¿Ÿåˆ°GPUä¸Šè½¬æ¢
    # åªåˆ›å»ºmodality_masksï¼ˆnumpyæ ¼å¼ï¼‰ï¼Œtensorè½¬æ¢åœ¨training_stepä¸­è¿›è¡Œ
    from purrsight.config import Modality
    
    # åˆ›å»ºmodality_masksï¼ˆnumpyæ ¼å¼ï¼‰
    modality_masks_numpy = {}
    for modality in [Modality.TEXT, Modality.IMAGE, Modality.AUDIO]:
        modality_key = modality.value
        if modality == Modality.TEXT:
            has_modality = (
                FeatureKey.TEXT in merged_batch_features
                and FeatureKey.TEXT_ATTENTION_MASK in merged_batch_features
            )
            if has_modality:
                mask = np.any(merged_batch_features[FeatureKey.TEXT_ATTENTION_MASK] != 0, axis=1)
            else:
                mask = np.zeros(batch_size, dtype=np.bool_)
        elif modality == Modality.IMAGE:
            has_modality = FeatureKey.IMAGE in merged_batch_features
            if has_modality:
                img_feat = merged_batch_features[FeatureKey.IMAGE]
                
                # ğŸ”§ è°ƒè¯•æ—¥å¿—ï¼šè®°å½•img_featå½¢çŠ¶
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ¡ä»¶åˆ¤æ–­é¿å…å­—ç¬¦ä¸²æ ¼å¼åŒ–å¼€é”€
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"collate_batch IMAGE maskè®¡ç®—: img_feat.shape={img_feat.shape}, "
                        f"batch_size={batch_size}, ndim={img_feat.ndim}"
                    )
                
                # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ å½¢çŠ¶éªŒè¯ï¼Œç¡®ä¿batchç»´åº¦æ­£ç¡®
                if img_feat.ndim == 5:
                    # æœŸæœ›æ ¼å¼: (B, 16, 3, 224, 224)
                    if img_feat.shape[0] != batch_size:
                        raise ValueError(
                            f"IMAGEç‰¹å¾batchç»´åº¦ä¸åŒ¹é…: shape={img_feat.shape}, "
                            f"batch_size={batch_size}. å¯èƒ½æ˜¯collateé˜¶æ®µå¤„ç†é”™è¯¯ã€‚"
                        )
                    img_mask = np.sum(np.abs(img_feat), axis=(1, 2, 3, 4)) > 1e-6
                elif img_feat.ndim == 4:
                    # æœŸæœ›æ ¼å¼: (B, 3, 224, 224) - å•å¸§æ ¼å¼ï¼ˆä¸åº”è¯¥å‡ºç°ï¼Œåº”è¯¥å·²è½¬æ¢ä¸º16å¸§ï¼‰
                    if img_feat.shape[0] != batch_size:
                        raise ValueError(
                            f"IMAGEç‰¹å¾batchç»´åº¦ä¸åŒ¹é…: shape={img_feat.shape}, "
                            f"batch_size={batch_size}. å¯èƒ½æ˜¯collateé˜¶æ®µå¤„ç†é”™è¯¯ã€‚"
                        )
                    img_mask = np.sum(np.abs(img_feat), axis=(1, 2, 3)) > 1e-6
                else:
                    # å…¶ä»–ç»´åº¦ï¼šåŠ¨æ€è®¡ç®—
                    if img_feat.shape[0] != batch_size:
                        raise ValueError(
                            f"IMAGEç‰¹å¾batchç»´åº¦ä¸åŒ¹é…: shape={img_feat.shape}, "
                            f"batch_size={batch_size}. å¯èƒ½æ˜¯collateé˜¶æ®µå¤„ç†é”™è¯¯ã€‚"
                        )
                    sum_axes = tuple(range(1, img_feat.ndim))
                    img_mask = np.sum(np.abs(img_feat), axis=sum_axes) > 1e-6
                
                # éªŒè¯img_maskå½¢çŠ¶
                if img_mask.shape != (batch_size,):
                    raise ValueError(
                        f"img_maskå½¢çŠ¶é”™è¯¯: shape={img_mask.shape}, æœŸæœ›({batch_size},). "
                        f"img_featå½¢çŠ¶={img_feat.shape}"
                    )
                
                # æ£€æŸ¥video_metadataå¹¶éªŒè¯ä¸€è‡´æ€§
                if video_metadata_batch:
                    video_mask = np.array([
                        idx in video_metadata_batch and video_metadata_batch[idx].get("has_video", False)
                        for idx in range(batch_size)
                    ], dtype=np.bool_)
                    
                    # éªŒè¯video_maskå½¢çŠ¶
                    if video_mask.shape != (batch_size,):
                        raise ValueError(
                            f"video_maskå½¢çŠ¶é”™è¯¯: shape={video_mask.shape}, æœŸæœ›({batch_size},)"
                        )
                    
                    # ğŸ”§ ä¿®å¤ï¼šéªŒè¯maskä¸video_metadataçš„ä¸€è‡´æ€§
                    for idx, meta in video_metadata_batch.items():
                        if not meta.get("image_valid", True):
                            img_mask[idx] = False
                    mask = video_mask | img_mask
                else:
                    mask = img_mask
            else:
                mask = np.zeros(batch_size, dtype=np.bool_)
        elif modality == Modality.AUDIO:
            has_modality = FeatureKey.AUDIO in merged_batch_features
            if has_modality:
                # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„maskè®¡ç®—
                audio_feat = merged_batch_features[FeatureKey.AUDIO]
                mask = np.sum(np.abs(audio_feat), axis=tuple(range(1, audio_feat.ndim))) > 1e-6
                # ğŸ”§ ä¿®å¤ï¼šéªŒè¯maskä¸video_metadataçš„ä¸€è‡´æ€§
                if video_metadata_batch:
                    from purrsight.config import ModalitySource
                    for idx, meta in video_metadata_batch.items():
                        if not meta.get("audio_valid", True):
                            audio_source = meta.get("audio_source")
                            if audio_source == ModalitySource.VIDEO.value:
                                mask[idx] = False
            else:
                mask = np.zeros(batch_size, dtype=np.bool_)
        
        modality_masks_numpy[modality_key] = mask.astype(np.bool_)
    
    # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šè¿”å›numpyæ ¼å¼ï¼Œå»¶è¿Ÿåˆ°GPUä¸Šè½¬æ¢ä¸ºtensor
    # è¿™æ ·å¯ä»¥é¿å…åœ¨CPUä¸Šå ç”¨å¤§é‡å†…å­˜
    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šåŒæ—¶è¿”å›batch_sizeï¼Œé¿å…åœ¨training_stepä¸­é‡å¤æ¨æ–­
    return merged_batch_features, modality_masks_numpy, batch_size


def train_loop_per_worker(rank: int, world_size: int, config: AlignmentConfig):
    """
    è®­ç»ƒå¾ªç¯ï¼ˆæ¯ä¸ªworkeræ‰§è¡Œï¼‰

    Args:
        rank: å½“å‰è¿›ç¨‹rankï¼ˆDDPä½¿ç”¨ï¼‰
        world_size: æ€»è¿›ç¨‹æ•°ï¼ˆDDPä½¿ç”¨ï¼‰
        config: è®­ç»ƒé…ç½®
    """
    # è®¾ç½®éšæœºç§å­ï¼ˆä¿è¯å¤šGPUä¸€è‡´æ€§ï¼‰
    pl.seed_everything(42 + rank)

    # è®¾ç½®è®¾å¤‡
    if world_size > 1:
        # DDPæ¨¡å¼ï¼ˆä»…æ”¯æŒCUDAï¼‰
        torch.cuda.set_device(rank)
        device = f"cuda:{rank}"
    else:
        # å•GPU/CPU/MPSæ¨¡å¼
        if config.device == "auto":
            from purrsight.utils.tools import get_available_device
            device = get_available_device()  # è‡ªåŠ¨æ£€æµ‹ï¼šMPS > CUDA > CPU
        else:
            device = config.device

    logger.info(f"Worker {rank}/{world_size} using device: {device}")

    # ç¦»çº¿é¢„å¤„ç†æ—¶ä» preprocessed_dir/index.jsonl åŠ è½½æ ·æœ¬åˆ—è¡¨ï¼›åœ¨çº¿æ—¶ä» data_path åŠ è½½
    data_path_to_load = config.data_path
    if config.use_preprocessed:
        if not config.preprocessed_dir:
            raise ValueError("use_preprocessed=True æ—¶å¿…é¡»é…ç½® preprocessed_dir")
        preprocessed_path = Path(config.preprocessed_dir.strip().strip('"')).resolve()
        if not preprocessed_path.exists():
            raise FileNotFoundError(
                f"é¢„å¤„ç†ç›®å½•ä¸å­˜åœ¨: {preprocessed_path}\n"
                f"è¯·å…ˆè¿è¡Œç¦»çº¿é¢„å¤„ç†ï¼špython -m purrsight.preprocess.prepre "
                f"--input_file <åŸå§‹æ•°æ®> --output_dir {config.preprocessed_dir}"
            )
        index_path = preprocessed_path / "index.jsonl"
        if index_path.exists() and index_path.stat().st_size > 0:
            data_path_to_load = str(index_path)
            logger.info(f"ä½¿ç”¨ç¦»çº¿é¢„å¤„ç†ï¼Œä»ç´¢å¼•åŠ è½½: {data_path_to_load}")
        else:
            raise FileNotFoundError(
                f"é¢„å¤„ç†ç›®å½•ä¸‹æœªæ‰¾åˆ°æœ‰æ•ˆç´¢å¼•: {index_path}\n"
                f"è¯·å…ˆå®Œæˆç¦»çº¿é¢„å¤„ç†å¹¶ç”Ÿæˆ index.jsonl"
            )

    # åŠ è½½æ•°æ®ï¼šå…ˆæ£€æŸ¥æ–‡ä»¶å­˜åœ¨ï¼Œé¿å…é™é»˜è¯»ç©º
    data_path_resolved = Path(data_path_to_load).resolve()
    if not data_path_resolved.exists():
        raise FileNotFoundError(
            f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path_resolved}\n"
            f"åœ¨çº¿æ¨¡å¼è¯·æ£€æŸ¥ config.data_pathï¼›ç¦»çº¿æ¨¡å¼è¯·æ£€æŸ¥ preprocessed_dir ä¸‹æ˜¯å¦æœ‰ index.jsonl"
        )
    if not data_path_resolved.is_file():
        raise FileNotFoundError(f"æ•°æ®è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {data_path_resolved}")

    logger.info(f"Loading data from {data_path_resolved} (size={data_path_resolved.stat().st_size} bytes)")
    if data_path_to_load.endswith('.jsonl'):
        data_list = load_data_from_jsonl(str(data_path_resolved))
    else:
        raise ValueError(f"Unsupported data format: {data_path_to_load}")

    logger.info(f"Loaded {len(data_list)} samples")
    if len(data_list) == 0:
        raise ValueError(
            "æ•°æ®åˆ—è¡¨ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ data_path æˆ– preprocessed_dir/index.jsonl æ˜¯å¦æœ‰æœ‰æ•ˆæ ·æœ¬ï¼ˆéç©ºè¡Œä¸”åˆæ³• JSONï¼‰"
        )
    # ä¾¿äºæ’æŸ¥ï¼šæ‰“å°é¦–æ¡æ ·æœ¬çš„ keysï¼Œç¡®è®¤æ ¼å¼
    sample_keys = list(data_list[0].keys()) if data_list else []
    logger.info(f"é¦–æ¡æ ·æœ¬å­—æ®µ: {sample_keys}")

    if config.use_preprocessed:
        logger.info(f"ç¦»çº¿é¢„å¤„ç†æ¨¡å¼ï¼Œé¢„å¤„ç†ç›®å½•: {config.preprocessed_dir}")
    else:
        logger.info("ä½¿ç”¨åœ¨çº¿é¢„å¤„ç†æ¨¡å¼ï¼ˆå®æ—¶é¢„å¤„ç†ï¼‰")

    # åˆ›å»ºæ•°æ®é›†
    preprocessed_dir_clean = Path(config.preprocessed_dir.strip().strip('"')).resolve() if config.preprocessed_dir else None
    dataset = AlignmentDataset(
        data_list=data_list,
        device="cpu",
        use_preprocessed=config.use_preprocessed,
        preprocessed_dir=preprocessed_dir_clean if config.use_preprocessed else None,
    )

    # åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†
    val_size = int(len(dataset) * config.val_split)
    train_size = len(dataset) - val_size

    train_dataset, val_dataset = random_split(
        dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    logger.info(f"Train set: {train_size} samples, Val set: {val_size} samples")

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    if train_size == 0:
        logger.error("è®­ç»ƒé›†ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ")
        return
    if val_size == 0:
        logger.warning("éªŒè¯é›†ä¸ºç©ºï¼Œå°†è·³è¿‡éªŒè¯")

    # åˆ›å»ºDataLoader
    # pin_memory: åªå¯¹CUDAå¯ç”¨ï¼ˆMPSä¸æ”¯æŒï¼ŒCPUä¸éœ€è¦ï¼‰
    # æ³¨æ„ï¼šMPSè®¾å¤‡è™½ç„¶ä¸æ”¯æŒpin_memoryï¼Œä½†æ•°æ®ä¼ è¾“ä»ç„¶é«˜æ•ˆ
    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨é…ç½®ä¸­çš„num_workers
    # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨deviceå­—ç¬¦ä¸²æ£€æŸ¥ï¼Œå…¼å®¹å­—ç¬¦ä¸²å’Œtorch.deviceå¯¹è±¡
    device_str = str(device) if isinstance(device, torch.device) else device
    
    # è‡ªåŠ¨è®¾ç½®prefetch_factorå’Œpersistent_workers
    use_workers = config.num_workers > 0
    persistent_workers = use_workers
    prefetch_factor = 2 if use_workers else None
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        collate_fn=collate_batch,
        pin_memory=(device_str.startswith("cuda")),  # åªå¯¹CUDAå¯ç”¨
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        collate_fn=collate_batch,
        pin_memory=(device_str.startswith("cuda")),  # åªå¯¹CUDAå¯ç”¨
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor
    )

    # åˆ›å»ºLightningModule
    model = ContrastiveAlignmentModule(config)

    # è·å–MLflow runä¿¡æ¯ï¼ˆç”¨äºç»Ÿä¸€å‘½åï¼‰
    active_run = mlflow.active_run()
    if active_run is not None:
        run_id = active_run.info.run_id
        run_name = active_run.info.run_name
        # ä»run_nameä¸­æå–æ—¶é—´æˆ³ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # run_nameæ ¼å¼ï¼šexperiment_name_YYYYMMDD_HHMMSS
        if "_" in run_name:
            parts = run_name.rsplit("_", 2)
            if len(parts) == 3:
                timestamp = f"{parts[1]}_{parts[2]}"  # YYYYMMDD_HHMMSS
            else:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    else:
        run_id = None
        run_name = f"{config.experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ç»Ÿä¸€checkpointç›®å½•ï¼šä½¿ç”¨run_id_timestampæ ¼å¼ï¼Œä¸æ‰‹åŠ¨ä¿å­˜çš„checkpointä¸€è‡´
    # è¿™æ ·Lightningçš„è‡ªåŠ¨checkpointå’Œæ‰‹åŠ¨checkpointéƒ½åœ¨åŒä¸€ä¸ªç›®å½•ç»“æ„ä¸‹
    if run_id is not None:
        checkpoint_base_dir = Path(config.save_dir) / f"{run_id}_{timestamp}"
    else:
        checkpoint_base_dir = Path(config.save_dir) / f"checkpoint_{timestamp}"
    
    checkpoint_base_dir.mkdir(parents=True, exist_ok=True)
    lightning_checkpoint_dir = checkpoint_base_dir / "lightning_checkpoints"
    lightning_checkpoint_dir.mkdir(parents=True, exist_ok=True)

    # ä½¿ç”¨å½“å‰ MLflow runï¼ˆä¸ train_model ä¸­ start_run ä¸€è‡´ï¼‰ï¼Œä¿è¯ params ä¸ metrics åœ¨åŒä¸€ runï¼Œæ›²çº¿å¯è§
    run_id_for_logger = None
    try:
        active_run = mlflow.active_run()
        run_id_for_logger = active_run.info.run_id if active_run else None
    except Exception:
        pass

    # è®¾ç½®MLflow loggerï¼šä¼ å…¥ run_id ä½¿ Lightning çš„æŒ‡æ ‡å†™å…¥åŒä¸€ runï¼›ä¸æ”¯æŒ run_id æ—¶é€€åŒ–ä¸º run_name
    try:
        if run_id_for_logger:
            mlf_logger = MLflowLogger(
                experiment_name=config.experiment_name,
                tracking_uri=config.mlflow_tracking_uri,
                run_name=run_name,
                run_id=run_id_for_logger,
            )
        else:
            mlf_logger = MLflowLogger(
                experiment_name=config.experiment_name,
                tracking_uri=config.mlflow_tracking_uri,
                run_name=run_name,
            )
    except TypeError:
        mlf_logger = MLflowLogger(
            experiment_name=config.experiment_name,
            tracking_uri=config.mlflow_tracking_uri,
            run_name=run_name,
        )

    # é…ç½®ModelCheckpointï¼šä¸ä¾èµ– monitorï¼Œé¿å… callback_metrics ä¸­æ—  train_loss æŠ¥é”™ï¼›æŒ‰å‘¨æœŸä¿å­˜ + lastï¼ŒæŒ‡æ ‡ä»…ç”¨ MLflow
    from pytorch_lightning.callbacks import ModelCheckpoint
    checkpoint_callback = ModelCheckpoint(
        dirpath=str(lightning_checkpoint_dir),
        filename="{epoch}-{step}",
        save_top_k=1,
        monitor=None,  # ä¸ç›‘å¬ metricï¼Œé¿å… Lightning æœªæ³¨å…¥ train_loss æ—¶æŠ¥é”™
        save_last=True,
        every_n_epochs=config.save_every,
        save_on_train_epoch_end=True,
    )
    
    # ğŸ”§ éªŒè¯4ï¼šæš‚æ—¶å…³é—­SpeedMonitorï¼ˆéªŒè¯SpeedMonitoræ˜¯å¦å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼‰
    # speed_monitor = SpeedMonitor(log_every_n_batches=config.log_every)  # ğŸ”§ éªŒè¯ï¼šä¸´æ—¶æ³¨é‡Š
    speed_monitor = None  # ğŸ”§ éªŒè¯ï¼šä¸´æ—¶ç¦ç”¨SpeedMonitor

    # æ£€æŸ¥DataLoaderé•¿åº¦
    logger.info(f"è®­ç»ƒDataLoaderé•¿åº¦: {len(train_loader)} batches")
    logger.info(f"éªŒè¯DataLoaderé•¿åº¦: {len(val_loader)} batches")
    logger.info(f"Checkpointç›®å½•: {checkpoint_base_dir}")
    logger.info(f"  - Lightningè‡ªåŠ¨checkpoint: {lightning_checkpoint_dir}")
    logger.info(f"  - æ‰‹åŠ¨ä¿å­˜checkpoint: {checkpoint_base_dir}")

    # åˆ›å»ºTrainer
    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šå¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰
    # ğŸ”§ ä¿®å¤ï¼šMPSè®¾å¤‡å¯¹mixed precisionçš„æ”¯æŒæœ‰é™ï¼Œå¦‚æœé‡åˆ°dtypeé”™è¯¯ï¼Œç¦ç”¨AMP
    # MPSè®¾å¤‡ï¼šæš‚æ—¶ç¦ç”¨AMPï¼ˆMPSå¯¹float16æ”¯æŒä¸å®Œå–„ï¼Œbackward passå¯èƒ½å¤±è´¥ï¼‰
    # CUDAè®¾å¤‡ï¼šä½¿ç”¨16-mixed
    # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨deviceå­—ç¬¦ä¸²æ£€æŸ¥ï¼Œå…¼å®¹å­—ç¬¦ä¸²å’Œtorch.deviceå¯¹è±¡
    device_str = str(device) if isinstance(device, torch.device) else device
    if device_str.startswith("mps"):
        # MPSè®¾å¤‡ï¼šç¦ç”¨AMPä»¥é¿å…backward passä¸­çš„dtypeé”™è¯¯
        precision = "32-true"  # ä½¿ç”¨float32
        logger.info("MPSè®¾å¤‡ï¼šç¦ç”¨mixed precisionä»¥é¿å…dtypeé”™è¯¯")
    elif device_str.startswith("cuda"):
        precision = "16-mixed"  # CUDAä½¿ç”¨float16
    else:
        precision = "32-true"  # CPUä½¿ç”¨float32
    
    trainer_kwargs = {
        "max_epochs": config.epochs,
        "logger": mlf_logger,
        # ğŸ”§ ä¿®å¤6ï¼šä¼˜åŒ–Lightningçš„log_every_n_stepsï¼ˆå¢åŠ åˆ°50ï¼Œå‡å°‘loggingé¢‘ç‡ï¼‰
        "log_every_n_steps": max(config.log_every, 50),  # è‡³å°‘50æ­¥æ‰logä¸€æ¬¡ï¼Œå‡å°‘MLflow I/Oé˜»å¡
        "callbacks": [checkpoint_callback] + ([speed_monitor] if speed_monitor is not None else []),  # ğŸ”§ éªŒè¯ï¼šå¦‚æœspeed_monitorä¸ºNoneåˆ™ä¸æ·»åŠ 
        "enable_progress_bar": True,
        "enable_model_summary": True,
        "num_sanity_val_steps": 0,  # è·³è¿‡sanity checkingï¼Œé¿å…å°æ•°æ®é›†é—®é¢˜
        "limit_train_batches": 1.0,  # ä½¿ç”¨æ‰€æœ‰è®­ç»ƒbatches
        "limit_val_batches": 1.0,  # ä½¿ç”¨æ‰€æœ‰éªŒè¯batches
        "precision": precision,
    }

    if world_size > 1:
        # DDPé…ç½®ï¼ˆä»…æ”¯æŒCUDAï¼‰
        trainer_kwargs.update({
            "accelerator": "gpu",
            "devices": world_size,
            "strategy": "ddp",
        })
    else:
        # å•GPU/CPU/MPSé…ç½®
        # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨deviceå­—ç¬¦ä¸²æ£€æŸ¥ï¼Œå…¼å®¹å­—ç¬¦ä¸²å’Œtorch.deviceå¯¹è±¡
        device_str = str(device) if isinstance(device, torch.device) else device
        if device_str.startswith("cuda"):
            trainer_kwargs.update({
                "accelerator": "gpu",
                "devices": 1,
            })
        elif device_str == "mps":
            trainer_kwargs.update({
                "accelerator": "mps",
                "devices": 1,
            })
        else:
            trainer_kwargs.update({
                "accelerator": "cpu",
            })

    trainer = pl.Trainer(**trainer_kwargs)

    # å¼€å§‹è®­ç»ƒ
    logger.info("Starting training...")
    trainer.fit(model, train_loader, val_loader)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨MLflow run IDç»Ÿä¸€ç¼–å·ï¼‰
    # æ³¨æ„ï¼šcheckpoint_base_dirã€run_idã€timestampå·²ç»åœ¨train_loop_per_workerå¼€å§‹æ—¶åˆ›å»º
    if rank == 0:  # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜
        # ä½¿ç”¨å·²ç»åˆ›å»ºçš„checkpoint_base_dirï¼ˆä¸Lightning checkpointåœ¨åŒä¸€ç›®å½•ï¼‰
        # è¿™æ ·æ‰‹åŠ¨ä¿å­˜çš„checkpointå’ŒLightningè‡ªåŠ¨ä¿å­˜çš„checkpointéƒ½åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹
        # checkpoint_base_diræ ¼å¼ï¼š{run_id}_{timestamp}ï¼Œä¾‹å¦‚ï¼š2750355bd92c443b9d851249630300be_20260113_152025
        checkpoint_path = checkpoint_base_dir

        # ä¿å­˜Lightning checkpointï¼ˆæœ€ç»ˆæ¨¡å‹ï¼‰
        trainer.save_checkpoint(checkpoint_path / "model.ckpt")

        # ä¿å­˜aligneræƒé‡ï¼ˆä¾¿äºåç»­ä½¿ç”¨ï¼‰
        torch.save(model.aligner.state_dict(), checkpoint_path / "aligner.pt")

        # ä¿å­˜è®­ç»ƒé…ç½®ä¿¡æ¯ï¼ˆä¾¿äºåç»­æŸ¥çœ‹ï¼‰
        config_info = {
            "run_id": run_id,
            "timestamp": timestamp,
            "experiment_name": config.experiment_name,
            "config": config.__dict__,
        }
        with open(checkpoint_path / "config.json", 'w', encoding='utf-8') as f:
            json.dump(config_info, f, indent=2, ensure_ascii=False)

        logger.info(f"Final checkpoint saved to {checkpoint_path}")
        logger.info(f"  Run ID: {run_id if run_id else 'N/A'}")
        logger.info(f"  Timestamp: {timestamp}")
        logger.info(f"  Experiment: {config.experiment_name}")
        
        # åœ¨MLflowä¸­è®°å½•checkpointè·¯å¾„
        if active_run is not None:
            mlflow.log_param("final_checkpoint_path", str(checkpoint_path))
            mlflow.log_param("lightning_checkpoint_dir", str(checkpoint_path / "lightning_checkpoints"))
            
            # âœ… ä¿å­˜Artifactsåˆ°MLflow
            save_artifacts_to_mlflow(
                checkpoint_path=checkpoint_path,
                model=model,
                config=config,
                trainer=trainer,
                active_run=active_run
            )


def train_model(config: AlignmentConfig):
    """
    ä¸»è®­ç»ƒå‡½æ•°
    
    è®¾ç½®MLflow experimentå¹¶å¯åŠ¨è®­ç»ƒã€‚
    
    åŠŸèƒ½ï¼š
    1. è®¾ç½®MLflow tracking URIå’Œexperiment
    2. åŠ è½½æ•°æ®å¹¶åˆ›å»ºDatasetå’ŒDataLoader
    3. åˆå§‹åŒ–æ¨¡å‹å’ŒTrainer
    4. å¯åŠ¨è®­ç»ƒ
    5. ä¿å­˜artifactsåˆ°MLflow

    Args:
        config: è®­ç»ƒé…ç½®
    """
    # è®¾ç½®MLflow
    mlflow.set_tracking_uri(config.mlflow_tracking_uri)
    
    # è·å–æˆ–åˆ›å»ºå®éªŒï¼ˆä½¿ç”¨æ˜“è¯»çš„å®éªŒåç§°ï¼‰
    # æ³¨æ„ï¼šMLflowçš„å®éªŒIDæ˜¯è‡ªåŠ¨ç”Ÿæˆçš„æ•°å­—ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡å®éªŒåç§°æ¥è¯†åˆ«
    try:
        experiment = mlflow.get_experiment_by_name(config.experiment_name)
        if experiment is None:
            # åˆ›å»ºæ–°å®éªŒ
            experiment_id = mlflow.create_experiment(
                config.experiment_name,
                tags={"description": f"Alignment training experiment: {config.experiment_name}"}
            )
            logger.info(f"Created new MLflow experiment: {config.experiment_name} (ID: {experiment_id})")
        else:
            logger.info(f"Using existing MLflow experiment: {config.experiment_name} (ID: {experiment.experiment_id})")
    except Exception as e:
        logger.warning(f"Failed to get/create experiment: {e}, using default")
    
    mlflow.set_experiment(config.experiment_name)

    # å¯åŠ¨MLflow runï¼ˆMLflowLoggerä¼šæ£€æµ‹å¹¶ä½¿ç”¨è¿™ä¸ªrunï¼‰
    # ç”Ÿæˆæ˜“è¯»çš„runåç§°ï¼šåŒ…å«æ—¶é—´æˆ³å’Œå®éªŒåç§°
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"{config.experiment_name}_{run_timestamp}"
    with mlflow.start_run(run_name=run_name):
        # è‡ªåŠ¨æ£€æµ‹ç¦»çº¿é¢„å¤„ç†æ•°æ®
        if not config.use_preprocessed:
            # æ£€æŸ¥é»˜è®¤ä½ç½®æˆ–é…ç½®çš„ä½ç½®
            # å¦‚æœ preprocessed_dir ä¸º Noneï¼Œæ£€æŸ¥é»˜è®¤ä½ç½® data/preprocessed
            check_dir = Path(config.preprocessed_dir) if config.preprocessed_dir else (ROOT_DIR / "data" / "preprocessed")
            check_index = check_dir / "index.jsonl"
            
            # åªæœ‰å½“ç´¢å¼•æ–‡ä»¶å­˜åœ¨ä¸”åŒ…å«å†…å®¹æ—¶æ‰åˆ‡æ¢
            if check_dir.exists() and check_index.exists() and check_index.stat().st_size > 0:
                logger.info("=" * 40)
                logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°ç¦»çº¿é¢„å¤„ç†æ•°æ®: {check_index}")
                logger.info("æ ¹æ®ç”¨æˆ·ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨ç¦»çº¿æ•°æ®ä»¥åŠ é€Ÿè®­ç»ƒ")
                logger.info(f"  - åˆ‡æ¢æ¨¡å¼: åœ¨çº¿é¢„å¤„ç† -> ç¦»çº¿é¢„å¤„ç†")
                logger.info(f"  - é¢„å¤„ç†ç›®å½•: {check_dir}")
                logger.info(f"  - æ•°æ®æºé‡å®šå‘: {config.data_path} -> {check_index}")
                logger.info("=" * 40)
                
                config.use_preprocessed = True
                config.preprocessed_dir = str(check_dir)
                config.data_path = str(check_index)

        # è®°å½•é…ç½®å‚æ•°
        mlflow.log_params(config.__dict__)

        # è®°å½•ä»£ç ç‰ˆæœ¬ï¼ˆå¯é€‰ï¼‰
        try:
            import git
            repo = git.Repo(search_parent_directories=True)
            mlflow.set_tag("git_commit", repo.head.commit.hexsha)
            mlflow.set_tag("git_branch", repo.active_branch.name)
        except:
            pass

        # å¯åŠ¨è®­ç»ƒï¼ˆMLflowLoggerä¼šä½¿ç”¨å½“å‰çš„active runï¼‰
        train_loop_per_worker(rank=0, world_size=1, config=config)

        logger.info("Training completed!")


if __name__ == "__main__":
    import argparse
    import yaml
    
    parser = argparse.ArgumentParser(description='Phase 1 Alignment Training')
    parser.add_argument('--config', type=str, required=True, help='Path to config YAML file')
    args = parser.parse_args()
    
    # Load config from YAML
    with open(args.config, 'r') as f:
        config_dict = yaml.safe_load(f)
    
    # Extract phase1 config
    phase1_config = config_dict.get('phase1', {})
    
    # Create AlignmentConfig
    config = AlignmentConfig(
        data_path=phase1_config.get('data_path', 'data/phase1/online/train.jsonl'),
        batch_size=phase1_config.get('batch_size', 32),
        epochs=phase1_config.get('epochs', 10),
        learning_rate=phase1_config.get('learning_rate', 1e-3),
        weight_decay=phase1_config.get('weight_decay', 0.01),
        warmup_steps=phase1_config.get('warmup_steps', 1000),
        num_workers=phase1_config.get('num_workers', 4),
        val_split=phase1_config.get('val_split', 0.1),
        use_preprocessed=phase1_config.get('use_preprocessed', False),
        preprocessed_dir=phase1_config.get('preprocessed_dir'),
        input_dim=phase1_config.get('input_dim', 512),
        output_dim=phase1_config.get('output_dim', 512),
        use_temperature_scaling=phase1_config.get('use_temperature_scaling', True),
        experiment_name=phase1_config.get('experiment_name', 'alignment_training'),
        log_every=config_dict.get('common', {}).get('log_every', 100),
        save_every=config_dict.get('common', {}).get('save_every', 1),
        device=config_dict.get('common', {}).get('device', 'auto'),
    )

    # å¯åŠ¨è®­ç»ƒ
    train_model(config)