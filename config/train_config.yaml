# Purr-Sight Unified Training Configuration
# 监控约定：所有指标统一用 MLflow 记录；checkpoint 仅按 train_loss 保存（见 purrsight.config.CHECKPOINT_MONITOR）

# Common settings for all phases
common:
  seed: 42
  project_name: "Purr-Sight"
  output_dir: "outputs"
  device: "auto"  # auto, cuda, mps, cpu
  log_every: 50
  save_every: 1

# Phase 1: Contrastive Alignment Training
# Aligns Image/Audio/Text encoders into a shared space
phase1:
  experiment_name: "alignment_phase1_1e-3_v0"
  data_path: "/Users/physicsboy/Desktop/data_4_purr/data_formal_alin/align_v0.jsonl"  # Path to alignment dataset
  batch_size: 32
  epochs: 20
  learning_rate: 1.0e-3
  weight_decay: 0.01
  warmup_steps: 1000
  num_workers: 8
  val_split: 0.2
  use_preprocessed: true
  preprocessed_dir: "/Users/physicsboy/Desktop/data_4_purr/data_formal_alin/preprocessed"
  input_dim: 512
  output_dim: 512
  use_temperature_scaling: true

# Phase 2: Multimodal Instruction Tuning
# Connects aligned encoders to an LLM via a projector
phase2:
  experiment_name: "instruction_tuning_phase2"
  
  # Model Paths
  llm_model_path: "models/Qwen2.5-0.5B-Instruct"  # Example path, change to your downloaded LLM
  
  # CRITICAL: Path to Phase 1 'aligner.pt' file (NOT directory).
  # Example: "outputs/alignment_phase1_20260126_120000/checkpoints/aligner.pt"
  adapter_path: null 
  
  # Data
  data_path: "data/instruction/train.jsonl"
  batch_size: 4
  gradient_accumulation_steps: 4
  epochs: 5
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.03
  num_workers: 4
  max_length: 2048  # Max sequence length
  
  # Projector Settings
  projector:
    hidden_dim: 2048
    num_tokens: 4
    
  # LoRA Settings (for efficient fine-tuning)
  lora:
    enabled: true
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # 
    
  # Freeze Settings
  freeze_encoders: true  # Freeze the aligned encoders
  freeze_projector: false # Train the projector
  freeze_llm: false      # False if LoRA is enabled (LoRA freezes base model automatically)
